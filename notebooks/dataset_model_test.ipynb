{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) CCconfig            7) libfabric/1.18.0      13) StdEnv/2023       (\u001b[1;31mS\u001b[0m)\n",
      "  2) gentoo/2023   (\u001b[1;31mS\u001b[0m)   8) pmix/4.2.4            14) mii/1.1.2\n",
      "  3) \u001b[2mgcccore/.12.3\u001b[0m (H)   9) ucc/1.2.0             15) python/3.11.5     (\u001b[1;34mt\u001b[0m)\n",
      "  4) gcc/12.3      (\u001b[1;34mt\u001b[0m)  10) openmpi/4.1.5    (\u001b[1;31mm\u001b[0m)  16) ipykernel/2025a\n",
      "  5) hwloc/2.9.1        11) flexiblas/3.3.1       17) scipy-stack/2025a (\u001b[1;32mmath\u001b[0m)\n",
      "  6) ucx/1.14.1         12) blis/0.9.0            18) opencv/4.11.0     (\u001b[1;34mvis\u001b[0m)\n",
      "\n",
      "  Where:\n",
      "   \u001b[1;31mS\u001b[0m:     Module is Sticky, requires --force to unload or purge\n",
      "   \u001b[1;31mm\u001b[0m:     MPI implementations / ImplÃ©mentations MPI\n",
      "   \u001b[1;32mmath\u001b[0m:  Mathematical libraries / BibliothÃ¨ques mathÃ©matiques\n",
      "   \u001b[1;34mt\u001b[0m:     Tools for development / Outils de dÃ©veloppement\n",
      "   \u001b[1;34mvis\u001b[0m:   Visualisation software / Logiciels de visualisation\n",
      "   H:                Hidden Module\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.dataset import get_dataset\n",
    "from utils.losses import Lnorm, Lamm, LDetection  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.ops as ops\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamm init called\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "        \"root_dir\": \"/home/soroush1/scratch/eecs_project\",\n",
    "        \"batch_size\": 4,\n",
    "        \"num_workers\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "    }\n",
    "\n",
    "# Unpack config\n",
    "root_dir = config[\"root_dir\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "num_workers = config[\"num_workers\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"lr\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and loader\n",
    "dataloader = get_dataset(\n",
    "    root_dir=root_dir,\n",
    "    split=\"train\",\n",
    "    transform=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    " # Model\n",
    "model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=num_classes)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Losses\n",
    "l_norm = Lnorm()\n",
    "l_amm = Lamm()\n",
    "l_det = LDetection(num_classes=num_classes, num_bins=16, top_k=9)\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Inspecting `targets` structure:\n",
      "--- Sample 0 ---\n",
      "Image ID:         tensor([0])\n",
      "Original Size:    tensor([540, 960])\n",
      "Boxes shape:      torch.Size([82, 4])\n",
      "Labels shape:     torch.Size([82])\n",
      "Boxes:            tensor([[708., 471., 782., 504.],\n",
      "        [639., 425., 700., 471.],\n",
      "        [594., 399., 658., 450.],\n",
      "        [562., 390., 623., 428.],\n",
      "        [540., 372., 605., 405.],\n",
      "        [514., 333., 582., 368.],\n",
      "        [501., 317., 565., 348.],\n",
      "        [501., 299., 546., 327.],\n",
      "        [489., 284., 537., 311.],\n",
      "        [463., 262., 511., 291.],\n",
      "        [458., 252., 507., 274.],\n",
      "        [448., 242., 493., 262.],\n",
      "        [442., 230., 491., 249.],\n",
      "        [439., 214., 484., 235.],\n",
      "        [429., 208., 471., 227.],\n",
      "        [420., 199., 463., 219.],\n",
      "        [398., 188., 439., 206.],\n",
      "        [ 46., 391.,  60., 417.],\n",
      "        [421., 433., 495., 477.],\n",
      "        [369., 346., 433., 380.],\n",
      "        [398., 410., 470., 456.],\n",
      "        [394., 393., 464., 429.],\n",
      "        [377., 364., 448., 402.],\n",
      "        [357., 312., 415., 343.],\n",
      "        [359., 298., 413., 320.],\n",
      "        [348., 283., 391., 311.],\n",
      "        [345., 271., 397., 290.],\n",
      "        [340., 260., 400., 278.],\n",
      "        [340., 250., 392., 266.],\n",
      "        [332., 231., 386., 253.],\n",
      "        [323., 213., 368., 238.],\n",
      "        [317., 195., 362., 226.],\n",
      "        [316., 188., 352., 203.],\n",
      "        [308., 179., 352., 196.],\n",
      "        [345., 163., 382., 181.],\n",
      "        [384., 164., 410., 186.],\n",
      "        [ 43., 398.,  61., 415.],\n",
      "        [324., 167., 338., 173.],\n",
      "        [362., 143., 392., 161.],\n",
      "        [310., 150., 346., 165.],\n",
      "        [258., 375., 288., 418.],\n",
      "        [218., 348., 236., 376.],\n",
      "        [249., 297., 272., 327.],\n",
      "        [  0., 298.,  26., 332.],\n",
      "        [ 33., 279.,  57., 309.],\n",
      "        [ 47., 243.,  73., 271.],\n",
      "        [ 72., 202., 100., 230.],\n",
      "        [ 87., 125., 102., 140.],\n",
      "        [170., 113., 185., 126.],\n",
      "        [102.,  81., 115.,  94.],\n",
      "        [415., 207., 420., 216.],\n",
      "        [428., 197., 436., 210.],\n",
      "        [234., 391., 242., 408.],\n",
      "        [879., 256., 891., 275.],\n",
      "        [897., 225., 905., 242.],\n",
      "        [ 57., 341.,  68., 361.],\n",
      "        [ 55., 345.,  68., 364.],\n",
      "        [ 55., 313.,  65., 333.],\n",
      "        [ 53., 320.,  67., 333.],\n",
      "        [232., 236., 239., 250.],\n",
      "        [237., 236., 243., 250.],\n",
      "        [193., 235., 200., 248.],\n",
      "        [192., 241., 203., 252.],\n",
      "        [178., 234., 184., 244.],\n",
      "        [177., 237., 185., 249.],\n",
      "        [117., 202., 124., 213.],\n",
      "        [118., 206., 125., 218.],\n",
      "        [ 59., 211.,  68., 224.],\n",
      "        [ 86., 194.,  94., 211.],\n",
      "        [219., 118., 226., 128.],\n",
      "        [193.,  99., 199., 109.],\n",
      "        [137., 115., 143., 124.],\n",
      "        [109., 107., 113., 114.],\n",
      "        [109., 110., 114., 118.],\n",
      "        [ 87., 102.,  96., 115.],\n",
      "        [112., 100., 116., 109.],\n",
      "        [ 59., 280.,  70., 298.],\n",
      "        [ 89.,  81.,  99.,  91.],\n",
      "        [136., 119., 142., 127.],\n",
      "        [841., 487., 862., 513.],\n",
      "        [223., 345., 231., 356.],\n",
      "        [329., 161., 336., 172.]])\n",
      "Labels:           tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4,\n",
      "        4, 5, 4, 5, 4, 5, 5, 6, 4, 4, 4, 4, 9, 3, 4, 4, 4, 7, 4, 8, 4, 4, 5, 4,\n",
      "        4, 4, 1, 1, 2, 1, 1, 2, 3, 1, 3, 1, 1, 2, 9, 2, 9, 2, 9, 1, 1, 1, 9, 2,\n",
      "        2, 9, 8, 2, 1, 5, 9, 9, 2, 2])\n",
      "--- Sample 1 ---\n",
      "Image ID:         tensor([1])\n",
      "Original Size:    tensor([540, 960])\n",
      "Boxes shape:      torch.Size([85, 4])\n",
      "Labels shape:     torch.Size([85])\n",
      "Boxes:            tensor([[ 85., 386., 168., 428.],\n",
      "        [193., 356., 254., 400.],\n",
      "        [ 95., 451., 178., 503.],\n",
      "        [ 74., 487., 168., 540.],\n",
      "        [107., 520., 184., 540.],\n",
      "        [239., 429., 297., 485.],\n",
      "        [274., 507., 363., 540.],\n",
      "        [707., 236., 762., 262.],\n",
      "        [676., 246., 729., 278.],\n",
      "        [662., 260., 707., 285.],\n",
      "        [646., 269., 697., 295.],\n",
      "        [631., 221., 666., 242.],\n",
      "        [734., 154., 767., 179.],\n",
      "        [367., 298., 389., 334.],\n",
      "        [310., 380., 325., 407.],\n",
      "        [287., 384., 306., 413.],\n",
      "        [299., 380., 316., 405.],\n",
      "        [319., 373., 333., 400.],\n",
      "        [329., 372., 341., 398.],\n",
      "        [339., 370., 353., 395.],\n",
      "        [352., 365., 366., 391.],\n",
      "        [358., 365., 374., 388.],\n",
      "        [366., 361., 384., 386.],\n",
      "        [378., 356., 392., 380.],\n",
      "        [389., 351., 404., 375.],\n",
      "        [395., 346., 410., 374.],\n",
      "        [406., 346., 423., 368.],\n",
      "        [439., 329., 455., 355.],\n",
      "        [426., 335., 445., 357.],\n",
      "        [466., 314., 488., 341.],\n",
      "        [483., 313., 497., 336.],\n",
      "        [491., 312., 507., 333.],\n",
      "        [501., 308., 521., 328.],\n",
      "        [512., 302., 530., 324.],\n",
      "        [530., 296., 551., 315.],\n",
      "        [546., 293., 565., 311.],\n",
      "        [559., 288., 581., 305.],\n",
      "        [571., 278., 585., 300.],\n",
      "        [583., 274., 600., 295.],\n",
      "        [600., 270., 613., 290.],\n",
      "        [613., 267., 631., 281.],\n",
      "        [718., 180., 739., 202.],\n",
      "        [168., 393., 183., 417.],\n",
      "        [ 34., 406.,  51., 433.],\n",
      "        [  0., 176.,  10., 192.],\n",
      "        [ 37., 162.,  47., 177.],\n",
      "        [ 45., 163.,  55., 175.],\n",
      "        [694., 215., 711., 234.],\n",
      "        [702., 216., 709., 228.],\n",
      "        [818., 246., 826., 266.],\n",
      "        [821., 253., 831., 269.],\n",
      "        [ 26., 307.,  38., 327.],\n",
      "        [ 19., 334.,  32., 354.],\n",
      "        [ 13., 347.,  25., 366.],\n",
      "        [ 22., 382.,  40., 401.],\n",
      "        [ 26., 389.,  38., 408.],\n",
      "        [ 74., 367.,  88., 393.],\n",
      "        [ 84., 365.,  98., 389.],\n",
      "        [ 97., 361., 112., 385.],\n",
      "        [108., 355., 122., 381.],\n",
      "        [ 37., 358.,  52., 381.],\n",
      "        [233., 416., 243., 441.],\n",
      "        [231., 418., 238., 431.],\n",
      "        [199., 277., 211., 295.],\n",
      "        [200., 270., 208., 285.],\n",
      "        [ 14., 200.,  35., 215.],\n",
      "        [728., 216., 734., 232.],\n",
      "        [722., 223., 741., 234.],\n",
      "        [640., 248., 649., 268.],\n",
      "        [633., 254., 641., 269.],\n",
      "        [797., 158., 804., 176.],\n",
      "        [805., 160., 812., 178.],\n",
      "        [862., 182., 871., 196.],\n",
      "        [871., 186., 877., 197.],\n",
      "        [866., 190., 872., 199.],\n",
      "        [837., 157., 847., 172.],\n",
      "        [889., 154., 897., 170.],\n",
      "        [820., 159., 835., 173.],\n",
      "        [826., 163., 832., 173.],\n",
      "        [771., 165., 786., 189.],\n",
      "        [747., 171., 754., 186.],\n",
      "        [876., 167., 883., 182.],\n",
      "        [873., 171., 885., 183.],\n",
      "        [924., 178., 932., 190.],\n",
      "        [941., 192., 949., 209.]])\n",
      "Labels:           tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
      "        9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 8, 1, 1, 1, 1, 1, 7,\n",
      "        2, 1, 1, 1, 1, 1, 2, 3, 9, 9, 9, 9, 1, 1, 2, 1, 1, 9, 2, 9, 1, 1, 1, 1,\n",
      "        1, 2, 2, 1, 1, 9, 2, 8, 1, 1, 9, 2, 1])\n",
      "--- Sample 2 ---\n",
      "Image ID:         tensor([2])\n",
      "Original Size:    tensor([540, 960])\n",
      "Boxes shape:      torch.Size([39, 4])\n",
      "Labels shape:     torch.Size([39])\n",
      "Boxes:            tensor([[341., 218., 382., 235.],\n",
      "        [383., 217., 424., 234.],\n",
      "        [437., 215., 475., 231.],\n",
      "        [183., 225., 226., 242.],\n",
      "        [208., 303., 264., 326.],\n",
      "        [238., 222., 280., 239.],\n",
      "        [368., 276., 417., 296.],\n",
      "        [535., 212., 572., 227.],\n",
      "        [538., 204., 572., 217.],\n",
      "        [610., 206., 647., 223.],\n",
      "        [658., 207., 696., 221.],\n",
      "        [705., 205., 742., 219.],\n",
      "        [749., 204., 786., 218.],\n",
      "        [785., 257., 801., 267.],\n",
      "        [850., 202., 885., 215.],\n",
      "        [897., 262., 948., 283.],\n",
      "        [941., 200., 960., 213.],\n",
      "        [486., 202., 492., 213.],\n",
      "        [789., 252., 798., 264.],\n",
      "        [ 58., 300.,  67., 318.],\n",
      "        [495., 200., 500., 214.],\n",
      "        [561., 199., 566., 210.],\n",
      "        [635., 162., 640., 173.],\n",
      "        [829., 157., 834., 168.],\n",
      "        [308., 171., 314., 183.],\n",
      "        [383., 168., 390., 182.],\n",
      "        [378., 150., 384., 160.],\n",
      "        [166., 123., 170., 134.],\n",
      "        [203., 115., 208., 125.],\n",
      "        [  2., 233.,  51., 253.],\n",
      "        [ 38., 306.,  68., 317.],\n",
      "        [111., 228., 156., 245.],\n",
      "        [290., 218., 330., 236.],\n",
      "        [487., 211., 523., 228.],\n",
      "        [494., 268., 537., 288.],\n",
      "        [605., 276., 652., 297.],\n",
      "        [650., 273., 700., 296.],\n",
      "        [808., 200., 844., 216.],\n",
      "        [894., 197., 930., 214.]])\n",
      "Labels:           tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 9, 4, 4, 4, 2, 2, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5])\n",
      "--- Sample 3 ---\n",
      "Image ID:         tensor([3])\n",
      "Original Size:    tensor([ 765, 1360])\n",
      "Boxes shape:      torch.Size([57, 4])\n",
      "Labels shape:     torch.Size([57])\n",
      "Boxes:            tensor([[ 206.,  540.,  377.,  667.],\n",
      "        [ 250.,  553.,  427.,  693.],\n",
      "        [ 329.,  554.,  520.,  765.],\n",
      "        [ 430.,  573.,  640.,  765.],\n",
      "        [ 596.,  600.,  764.,  763.],\n",
      "        [ 757.,  626.,  881.,  765.],\n",
      "        [ 910.,  645., 1055.,  765.],\n",
      "        [1033.,  661., 1158.,  763.],\n",
      "        [ 537.,  363.,  655.,  437.],\n",
      "        [ 588.,  353.,  694.,  420.],\n",
      "        [ 624.,  337.,  743.,  403.],\n",
      "        [ 660.,  321.,  783.,  382.],\n",
      "        [ 719.,  310.,  837.,  372.],\n",
      "        [ 773.,  299.,  874.,  359.],\n",
      "        [ 801.,  265.,  906.,  345.],\n",
      "        [ 834.,  257.,  939.,  336.],\n",
      "        [ 872.,  246.,  969.,  327.],\n",
      "        [ 911.,  253., 1005.,  313.],\n",
      "        [ 947.,  248., 1034.,  306.],\n",
      "        [ 974.,  249., 1062.,  295.],\n",
      "        [ 991.,  245., 1082.,  292.],\n",
      "        [1002.,  243., 1103.,  280.],\n",
      "        [1101.,  232., 1148.,  268.],\n",
      "        [1083.,  237., 1127.,  276.],\n",
      "        [1102.,  224., 1168.,  263.],\n",
      "        [1109.,  222., 1188.,  260.],\n",
      "        [1116.,  199., 1205.,  253.],\n",
      "        [1133.,  190., 1221.,  247.],\n",
      "        [1159.,  187., 1238.,  236.],\n",
      "        [1239.,  184., 1315.,  223.],\n",
      "        [1257.,  182., 1331.,  218.],\n",
      "        [1264.,  165., 1345.,  211.],\n",
      "        [1279.,  158., 1359.,  204.],\n",
      "        [1312.,  164., 1360.,  199.],\n",
      "        [ 655.,  261.,  731.,  298.],\n",
      "        [ 815.,  410.,  953.,  497.],\n",
      "        [ 870.,  392., 1007.,  479.],\n",
      "        [ 907.,  376., 1053.,  455.],\n",
      "        [ 957.,  364., 1095.,  443.],\n",
      "        [ 997.,  333., 1089.,  397.],\n",
      "        [1044.,  341., 1156.,  422.],\n",
      "        [1105.,  372., 1225.,  450.],\n",
      "        [1170.,  393., 1299.,  477.],\n",
      "        [1246.,  417., 1360.,  513.],\n",
      "        [1213.,  247., 1318.,  308.],\n",
      "        [1245.,  308., 1360.,  385.],\n",
      "        [1288.,  267., 1359.,  312.],\n",
      "        [1321.,  252., 1360.,  291.],\n",
      "        [ 853.,  205.,  933.,  255.],\n",
      "        [ 895.,  208.,  962.,  249.],\n",
      "        [ 924.,  199., 1000.,  242.],\n",
      "        [ 958.,  194., 1034.,  237.],\n",
      "        [ 985.,  187., 1072.,  233.],\n",
      "        [1011.,  152., 1106.,  223.],\n",
      "        [1052.,  169., 1116.,  213.],\n",
      "        [1156.,  162., 1225.,  192.],\n",
      "        [1171.,  155., 1247.,  187.]])\n",
      "Labels:           tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 6, 6, 4, 6, 6, 6, 5,\n",
      "        6, 6, 6, 6, 6, 6, 6, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "images = batch[\"image\"].to(device)\n",
    "targets = {\n",
    "    \"boxes\": batch[\"boxes\"],\n",
    "    \"labels\": [lbl.clamp(0, num_classes - 1) for lbl in batch[\"labels\"]],\n",
    "    \"image_id\": batch[\"image_id\"],\n",
    "    \"orig_size\": batch[\"orig_size\"],\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ” Inspecting `targets` structure:\")\n",
    "for i in range(len(targets[\"boxes\"])):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "    print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "    print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "    print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "    print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "    print(f\"Labels:           {targets['labels'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "outputs = model(images, stage=\"train\")\n",
    "(\n",
    "    cls_outs,\n",
    "    reg_outs,\n",
    "    soft_mask_outs,\n",
    "    sparse_cls_feats_outs,\n",
    "    sparse_reg_feats_outs,\n",
    "    dense_cls_feats_outs,\n",
    "    dense_reg_feats_outs,\n",
    "    feats,\n",
    "    anchors,\n",
    ") = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nðŸ” Output shapes from model:\")\n",
    "for i in range(len(cls_outs)):\n",
    "    print(f\"--- FPN Level {i} ---\")\n",
    "    print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\")\n",
    "    print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\")\n",
    "    print(\n",
    "        f\"cls_soft_mask_outs[{i}]:    {safe_shape(cls_soft_mask_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"reg_soft_mask_outs[{i}]:    {safe_shape(reg_soft_mask_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "    )\n",
    "    print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(f\"P{i+3} Anchors shape: {anchor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_per_sample = []\n",
    "\n",
    "for i in range(len(targets[\"boxes\"])):\n",
    "    boxes = [targets[\"boxes\"][i].to(device)]  # keep list structure\n",
    "    im_h, im_w = targets[\"orig_size\"][i]\n",
    "    im_dim = (int(im_w.item()), int(im_h.item()))  # convert to (W, H)\n",
    "\n",
    "    # Extract per-sample soft mask for all levels\n",
    "    soft_mask_sample = [f[i].unsqueeze(0) for f in cls_soft_mask_outs]  # list of [1, 1, H, W]\n",
    "\n",
    "    loss_i = l_amm(soft_mask_sample, boxes, im_dim=im_dim)\n",
    "    losses_per_sample.append(loss_i)\n",
    "\n",
    "loss_amm = sum(losses_per_sample) / len(losses_per_sample)\n",
    "loss_amm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_norm = l_norm(dense_cls_feats_outs, [[m]*4 for m in cls_soft_mask_outs], sparse_cls_feats_outs)\n",
    "loss_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anchors_on_image(image_tensor, anchors, num_to_plot=100, title=\"Anchors\", color=\"red\"):\n",
    "    \"\"\"\n",
    "    Plots anchor boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (Tensor): shape (3, H, W)\n",
    "        anchors (Tensor): shape (N, 4), format (x1, y1, x2, y2)\n",
    "        num_to_plot (int): number of anchor boxes to plot\n",
    "        title (str): title of the plot\n",
    "        color (str): color of anchor boxes\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy for visualization\n",
    "    if isinstance(image_tensor, torch.Tensor):\n",
    "        # If image is a tensor (transformed), convert back to numpy\n",
    "        img = image_tensor.permute(1, 2, 0).detach().cpu().numpy()\n",
    "        # Unnormalize if normalized\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = img * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "    \n",
    "    image = TF.to_pil_image(image_tensor.cpu())\n",
    "    anchors_np = anchors.cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for i in range(min(num_to_plot, len(anchors_np))):\n",
    "        x1, y1, x2, y2 = anchors_np[i]\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            linewidth=1,\n",
    "            edgecolor=color,\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "ds = dataloader.dataset\n",
    "plot_anchors_on_image(images[3], anchors[0], num_to_plot=5000, title=\"Anchors at FPN Level 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_item(image_tensor, boxes, figsize=(10, 10)):\n",
    "        \"\"\"\n",
    "        Visualize an image with its annotations\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to visualize\n",
    "            figsize (tuple): Figure size\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.patches as patches\n",
    "        from matplotlib.colors import to_rgba\n",
    "\n",
    "        # Convert tensor to numpy for visualization\n",
    "        if isinstance(image_tensor, torch.Tensor):\n",
    "            # If image is a tensor (transformed), convert back to numpy\n",
    "            img = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            # Unnormalize if normalized\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = img * std + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "        else:\n",
    "            # If image is PIL, convert to numpy\n",
    "            img = np.array(sample[\"image\"]) / 255.0\n",
    "\n",
    "        # Create figure and axis\n",
    "        fig, ax = plt.subplots(1, figsize=figsize)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Define colors for different categories (you can customize these)\n",
    "        colors = [\n",
    "            \"red\",\n",
    "            \"blue\",\n",
    "            \"green\",\n",
    "            \"yellow\",\n",
    "            \"purple\",\n",
    "            \"orange\",\n",
    "            \"cyan\",\n",
    "            \"magenta\",\n",
    "            \"brown\",\n",
    "            \"pink\",\n",
    "        ]\n",
    "\n",
    "        # Plot bounding boxes\n",
    "        for box in boxes:\n",
    "            # print(f\"{box.size() = }\")\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "\n",
    "            # Get color based on category\n",
    "            # color = colors[(label - 1) % len(colors)]\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # plt.title(f\"Image: {sample['img_name']} - {len(boxes)} objects\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # save instead of show\n",
    "        plt.savefig(\"test.png\")\n",
    "        plt.close()\n",
    "\n",
    "visualize_item(images[3], targets[\"boxes\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[\"boxes\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models.task_modules.prior_generators import AnchorGenerator\n",
    "import torch\n",
    "\n",
    "img_height, img_width = 765, 1360\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "anchor_generator = AnchorGenerator(\n",
    "    strides=[4, 8, 16, 32, 64],\n",
    "    ratios=[0.5, 1.0, 2.0],\n",
    "    scales=[8, 16],\n",
    "    base_sizes=[16, 32, 64, 128, 256],\n",
    ")\n",
    "\n",
    "# Fix: extract stride from tuple\n",
    "feature_map_sizes = [\n",
    "    (img_height // s[0], img_width // s[1]) for s in anchor_generator.strides\n",
    "]\n",
    "\n",
    "# Generate anchors in image space\n",
    "multi_level_anchors = anchor_generator.grid_priors(\n",
    "    featmap_sizes=feature_map_sizes,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "all_anchors = torch.cat(multi_level_anchors, dim=0)\n",
    "\n",
    "print(\"Total anchors:\", all_anchors.shape)\n",
    "print(\"Sample anchors:\\n\", all_anchors[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemeting Detection loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATSSMatcher:\n",
    "    def __init__(self, top_k=9):\n",
    "        self.top_k = top_k  # number of anchors to select per level\n",
    "\n",
    "    def __call__(self, anchors_per_level, gt_boxes, device = None):\n",
    "        \"\"\"\n",
    "        anchors_per_level: List[Tensor[N_i, 4]] in (x1, y1, x2, y2) format\n",
    "        gt_boxes: Tensor[M, 4]\n",
    "        Returns:\n",
    "            matched_idxs: Tensor[N_total] with GT index or -1\n",
    "            max_ious: Tensor[N_total]\n",
    "        \"\"\"\n",
    "        \n",
    "        num_gt = gt_boxes.size(0)\n",
    "        all_anchors = torch.cat(anchors_per_level, dim=0)  # [N_total, 4]\n",
    "        num_anchors = all_anchors.size(0)\n",
    "\n",
    "        if device:\n",
    "            all_anchors = all_anchors.to(device)\n",
    "            gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "        matched_idxs = torch.full((num_anchors,), -1, dtype=torch.long, device=gt_boxes.device)\n",
    "        max_ious = torch.zeros(num_anchors, dtype=torch.float, device=gt_boxes.device)\n",
    "\n",
    "        # 1. Compute IoU between all anchors and GTs\n",
    "        ious = ops.box_iou(all_anchors, gt_boxes)  # [N_total, M]\n",
    "\n",
    "        # 2. Compute anchor centers\n",
    "        anchor_centers = (all_anchors[:, :2] + all_anchors[:, 2:]) / 2  # [N, 2]\n",
    "        gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # [M, 2]\n",
    "\n",
    "        for gt_idx in range(num_gt):\n",
    "            gt_box = gt_boxes[gt_idx]\n",
    "            gt_center = gt_centers[gt_idx]  # [2]\n",
    "\n",
    "            # Distance from GT center to anchor centers\n",
    "            distances = torch.norm(anchor_centers - gt_center[None, :], dim=1)  # [N]\n",
    "\n",
    "            # Pick top-k closest anchors\n",
    "            topk_idxs = torch.topk(distances, self.top_k, largest=False).indices  # [top_k]\n",
    "\n",
    "            topk_ious = ious[topk_idxs, gt_idx]\n",
    "            iou_mean = topk_ious.mean()\n",
    "            iou_std = topk_ious.std()\n",
    "            dynamic_thresh = iou_mean + iou_std\n",
    "\n",
    "            # Positive = anchors with IoU >= dynamic_thresh and inside GT\n",
    "            candidate_mask = ious[:, gt_idx] >= dynamic_thresh\n",
    "\n",
    "            inside_gt = self.anchor_inside_box(all_anchors, gt_box)\n",
    "            pos_mask = candidate_mask & inside_gt  # [N]\n",
    "\n",
    "            pos_indices = pos_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "            matched_idxs[pos_indices] = gt_idx\n",
    "            max_ious[pos_indices] = ious[pos_indices, gt_idx]\n",
    "\n",
    "        return matched_idxs, max_ious\n",
    "\n",
    "    def anchor_inside_box(self, anchors, gt_box):\n",
    "        \"\"\"\n",
    "        Return a mask of anchors whose center is inside the GT box.\n",
    "        \"\"\"\n",
    "        cx = (anchors[:, 0] + anchors[:, 2]) / 2\n",
    "        cy = (anchors[:, 1] + anchors[:, 3]) / 2\n",
    "\n",
    "        return (\n",
    "            (cx >= gt_box[0]) & (cx <= gt_box[2]) &\n",
    "            (cy >= gt_box[1]) & (cy <= gt_box[3])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([534138]), torch.Size([534138]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = ATSSMatcher(top_k=9)\n",
    "matched_idxs = []\n",
    "max_ious = []\n",
    "\n",
    "for batch_id in range(len(targets[\"boxes\"])):\n",
    "    \n",
    "    matched_idx, iou = matcher(anchors, targets[\"boxes\"][batch_id], device=device)  # for image i\n",
    "    matched_idxs.append(matched_idx)\n",
    "    max_ious.append(iou)\n",
    "\n",
    "matched_idxs[0].size(), max_ious[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 534138])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_anchors = torch.cat(anchors, dim=0)  # [N_total, 4]\n",
    "num_total_anchors = all_anchors.size(0)\n",
    "batch_size = len(targets[\"boxes\"])\n",
    "cls_targets = torch.zeros((batch_size, num_total_anchors), dtype=torch.long, device=device)\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "    # For positive anchors, assign the GT class label\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        gt_indices = matched_idxs[batch_idx][pos_mask]\n",
    "        target = targets[\"labels\"][batch_idx].to(device)\n",
    "        cls_targets[batch_idx, pos_mask] = target[gt_indices]\n",
    "\n",
    "cls_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 534138])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_targets = torch.zeros((batch_size, num_total_anchors), dtype=torch.float, device=device)\n",
    "for batch_idx in range(batch_size):\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        iou_targets[batch_idx, pos_mask] = max_ious[batch_idx][pos_mask]\n",
    "\n",
    "iou_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 534138, 4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_targets = torch.zeros((batch_size, num_total_anchors, 4), dtype=torch.float, device=device)\n",
    "for batch_idx in range(batch_size):\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        gt_indices = matched_idxs[batch_idx][pos_mask]\n",
    "        bbox = targets[\"boxes\"][batch_idx].to(device)\n",
    "        \n",
    "        gt_boxes_matched = bbox[gt_indices]\n",
    "        reg_targets[batch_idx, pos_mask] = gt_boxes_matched\n",
    "\n",
    "reg_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_pred.size() = torch.Size([4, 66800, 10])\n",
      "cls_pred.size() = torch.Size([4, 16700, 10])\n",
      "cls_pred.size() = torch.Size([4, 4200, 10])\n",
      "cls_pred.size() = torch.Size([4, 1050, 10])\n",
      "cls_pred.size() = torch.Size([4, 273, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 89023, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Model Predictions\n",
    "\n",
    "batch_size = cls_outs[0].shape[0]\n",
    "all_cls_preds = []\n",
    "\n",
    "for level_idx, cls_pred in enumerate(cls_outs):\n",
    "    # [B, C, H, W] -> [B, H, W, C] -> [B, H*W*A, C]\n",
    "    B, C, H, W = cls_pred.shape\n",
    "    cls_pred = cls_pred.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "    print(f\"{cls_pred.size() = }\")\n",
    "    all_cls_preds.append(cls_pred)\n",
    "    \n",
    "cls_preds = torch.cat(all_cls_preds, dim=1)  # [B, N, C]\n",
    "\n",
    "cls_preds.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
