{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.visdrone_dataloader import get_dataset\n",
    "from utils.losses import Lnorm, Lamm, LDet, DetectionLoss  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.ops as ops\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        \"root_dir\": \"/home/soroush1/scratch/eecs_project\",\n",
    "        \"batch_size\": 4,\n",
    "        \"num_workers\": 4,\n",
    "        \"num_epochs\": 1,\n",
    "        \"lr\": 1e-3,\n",
    "        \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "    }\n",
    "\n",
    "# Unpack config\n",
    "root_dir = config[\"root_dir\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "num_workers = config[\"num_workers\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"lr\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and loader\n",
    "dataloader = get_dataset(\n",
    "    root_dir=root_dir,\n",
    "    split=\"train\",\n",
    "    transform=None,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    " # Model\n",
    "model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=num_classes)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Losses\n",
    "l_norm = Lnorm()\n",
    "l_amm = Lamm()\n",
    "l_det = LDet(num_classes=num_classes, num_bins=16)\n",
    "\n",
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images = batch[\"image\"].to(device)\n",
    "targets = {\n",
    "    \"boxes\": batch[\"boxes\"],\n",
    "    \"labels\": [lbl.clamp(0, num_classes - 1) for lbl in batch[\"labels\"]],\n",
    "    \"image_id\": batch[\"image_id\"],\n",
    "    \"orig_size\": batch[\"orig_size\"],\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Inspecting `targets` structure:\")\n",
    "for i in range(len(targets[\"boxes\"])):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "    print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "    print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "    print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "    print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "    print(f\"Labels:           {targets['labels'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "outputs = model(images, stage=\"train\")\n",
    "(\n",
    "    cls_outs,\n",
    "    reg_outs,\n",
    "    soft_mask_outs,\n",
    "    sparse_cls_feats_outs,\n",
    "    sparse_reg_feats_outs,\n",
    "    dense_cls_feats_outs,\n",
    "    dense_reg_feats_outs,\n",
    "    feats,\n",
    "    anchors,\n",
    ") = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nüîç Output shapes from model:\")\n",
    "for i in range(len(cls_outs)):\n",
    "    print(f\"--- FPN Level {i} ---\")\n",
    "    print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\") # anchors * num_classes \n",
    "    print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\") # anchors * 4 * bin_numbers\n",
    "    print(\n",
    "        f\"soft_mask_outs[{i}]:    {safe_shape(soft_mask_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "    )\n",
    "    print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "\n",
    "for i, anchor in enumerate(anchors):\n",
    "    print(f\"P{i+3} Anchors shape: {anchor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models.losses import QualityFocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QualityFocalLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Calculate detection loss ===\n",
    "loss_fn = DetectionLoss(num_bins=16, num_classes=10, num_anchors=6)\n",
    "losses = loss_fn(cls_outs, reg_outs, anchors, targets, device=device)\n",
    "print(f\"{losses = }\")\n",
    "print(losses['total_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_amm = l_amm(\n",
    "        soft_mask_outs, targets[\"boxes\"], im_dimx=1024, im_dimy=540\n",
    "    )  # used the soft masks in this version, might be incorrect\n",
    "\n",
    "loss_amm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_norm = l_norm(\n",
    "                sparse_cls_feats_outs, soft_mask_outs, dense_cls_feats_outs\n",
    "            )\n",
    "\n",
    "loss_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 10.0\n",
    "\n",
    "final_loss = (\n",
    "    losses[\"total_loss\"] + \n",
    "    alpha * loss_norm + \n",
    "    beta * loss_amm\n",
    ")\n",
    "\n",
    "final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_anchors_on_image(image_tensor, anchors, num_to_plot=100, title=\"Anchors\", color=\"red\"):\n",
    "    \"\"\"\n",
    "    Plots anchor boxes on an image.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (Tensor): shape (3, H, W)\n",
    "        anchors (Tensor): shape (N, 4), format (x1, y1, x2, y2)\n",
    "        num_to_plot (int): number of anchor boxes to plot\n",
    "        title (str): title of the plot\n",
    "        color (str): color of anchor boxes\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy for visualization\n",
    "    if isinstance(image_tensor, torch.Tensor):\n",
    "        # If image is a tensor (transformed), convert back to numpy\n",
    "        img = image_tensor.permute(1, 2, 0).detach().cpu().numpy()\n",
    "        # Unnormalize if normalized\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = img * std + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "\n",
    "    \n",
    "    image = TF.to_pil_image(image_tensor.cpu())\n",
    "    anchors_np = anchors.cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for i in range(min(num_to_plot, len(anchors_np))):\n",
    "        x1, y1, x2, y2 = anchors_np[i]\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            linewidth=1,\n",
    "            edgecolor=color,\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "ds = dataloader.dataset\n",
    "plot_anchors_on_image(images[3], anchors[0], num_to_plot=5000, title=\"Anchors at FPN Level 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_item(image_tensor, boxes, figsize=(10, 10)):\n",
    "        \"\"\"\n",
    "        Visualize an image with its annotations\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to visualize\n",
    "            figsize (tuple): Figure size\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib.patches as patches\n",
    "        from matplotlib.colors import to_rgba\n",
    "\n",
    "        # Convert tensor to numpy for visualization\n",
    "        if isinstance(image_tensor, torch.Tensor):\n",
    "            # If image is a tensor (transformed), convert back to numpy\n",
    "            img = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "            # Unnormalize if normalized\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = img * std + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "        else:\n",
    "            # If image is PIL, convert to numpy\n",
    "            img = np.array(sample[\"image\"]) / 255.0\n",
    "\n",
    "        # Create figure and axis\n",
    "        fig, ax = plt.subplots(1, figsize=figsize)\n",
    "        ax.imshow(img)\n",
    "\n",
    "        # Define colors for different categories (you can customize these)\n",
    "        colors = [\n",
    "            \"red\",\n",
    "            \"blue\",\n",
    "            \"green\",\n",
    "            \"yellow\",\n",
    "            \"purple\",\n",
    "            \"orange\",\n",
    "            \"cyan\",\n",
    "            \"magenta\",\n",
    "            \"brown\",\n",
    "            \"pink\",\n",
    "        ]\n",
    "\n",
    "        # Plot bounding boxes\n",
    "        for box in boxes:\n",
    "            # print(f\"{box.size() = }\")\n",
    "            x1, y1, x2, y2 = box\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "\n",
    "            # Get color based on category\n",
    "            # color = colors[(label - 1) % len(colors)]\n",
    "\n",
    "            # Create rectangle\n",
    "            rect = patches.Rectangle(\n",
    "                (x1, y1), width, height, linewidth=2, edgecolor=\"red\", facecolor=\"none\"\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # plt.title(f\"Image: {sample['img_name']} - {len(boxes)} objects\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # save instead of show\n",
    "        plt.savefig(\"test.png\")\n",
    "        plt.close()\n",
    "\n",
    "visualize_item(images[3], targets[\"boxes\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemeting Detection loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATSSMatcher:\n",
    "    def __init__(self, top_k=9):\n",
    "        self.top_k = top_k  # number of anchors to select per level\n",
    "\n",
    "    def __call__(self, anchors_per_level, gt_boxes, device = None):\n",
    "        \"\"\"\n",
    "        anchors_per_level: List[Tensor[N_i, 4]] in (x1, y1, x2, y2) format\n",
    "        gt_boxes: Tensor[M, 4]\n",
    "        Returns:\n",
    "            matched_idxs: Tensor[N_total] with GT index or -1\n",
    "            max_ious: Tensor[N_total]\n",
    "        \"\"\"\n",
    "        \n",
    "        num_gt = gt_boxes.size(0)\n",
    "        all_anchors = torch.cat(anchors_per_level, dim=0)  # [N_total, 4]\n",
    "        num_anchors = all_anchors.size(0)\n",
    "\n",
    "        if device:\n",
    "            all_anchors = all_anchors.to(device)\n",
    "            gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "        matched_idxs = torch.full((num_anchors,), -1, dtype=torch.long, device=gt_boxes.device)\n",
    "        max_ious = torch.zeros(num_anchors, dtype=torch.float, device=gt_boxes.device)\n",
    "\n",
    "        # 1. Compute IoU between all anchors and GTs\n",
    "        ious = ops.box_iou(all_anchors, gt_boxes)  # [N_total, M]\n",
    "\n",
    "        # 2. Compute anchor centers\n",
    "        anchor_centers = (all_anchors[:, :2] + all_anchors[:, 2:]) / 2  # [N, 2]\n",
    "        gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # [M, 2]\n",
    "\n",
    "        for gt_idx in range(num_gt):\n",
    "            gt_box = gt_boxes[gt_idx]\n",
    "            gt_center = gt_centers[gt_idx]  # [2]\n",
    "\n",
    "            # Distance from GT center to anchor centers\n",
    "            distances = torch.norm(anchor_centers - gt_center[None, :], dim=1)  # [N]\n",
    "\n",
    "            # Pick top-k closest anchors\n",
    "            topk_idxs = torch.topk(distances, self.top_k, largest=False).indices  # [top_k]\n",
    "\n",
    "            topk_ious = ious[topk_idxs, gt_idx]\n",
    "            iou_mean = topk_ious.mean()\n",
    "            iou_std = topk_ious.std()\n",
    "            dynamic_thresh = iou_mean + iou_std\n",
    "\n",
    "            # Positive = anchors with IoU >= dynamic_thresh and inside GT\n",
    "            candidate_mask = ious[:, gt_idx] >= dynamic_thresh\n",
    "\n",
    "            inside_gt = self.anchor_inside_box(all_anchors, gt_box)\n",
    "            pos_mask = candidate_mask & inside_gt  # [N]\n",
    "\n",
    "            pos_indices = pos_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "            matched_idxs[pos_indices] = gt_idx\n",
    "            max_ious[pos_indices] = ious[pos_indices, gt_idx]\n",
    "\n",
    "        return matched_idxs, max_ious\n",
    "\n",
    "    def anchor_inside_box(self, anchors, gt_box):\n",
    "        \"\"\"\n",
    "        Return a mask of anchors whose center is inside the GT box.\n",
    "        \"\"\"\n",
    "        cx = (anchors[:, 0] + anchors[:, 2]) / 2\n",
    "        cy = (anchors[:, 1] + anchors[:, 3]) / 2\n",
    "\n",
    "        return (\n",
    "            (cx >= gt_box[0]) & (cx <= gt_box[2]) &\n",
    "            (cy >= gt_box[1]) & (cy <= gt_box[3])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = ATSSMatcher(top_k=9)\n",
    "matched_idxs = []\n",
    "max_ious = []\n",
    "\n",
    "for batch_id in range(len(targets[\"boxes\"])):\n",
    "    \n",
    "    matched_idx, iou = matcher(anchors, targets[\"boxes\"][batch_id], device=device)  # for image i\n",
    "    matched_idxs.append(matched_idx)\n",
    "    max_ious.append(iou)\n",
    "\n",
    "\n",
    "matched_idxs = torch.stack(matched_idxs, dim=0)\n",
    "max_ious = torch.stack(max_ious, dim=0)\n",
    "\n",
    "matched_idxs.size(), max_ious.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anchors = torch.cat(anchors, dim=0)  # [N_total, 4]\n",
    "num_total_anchors = all_anchors.size(0)\n",
    "batch_size = len(targets[\"boxes\"])\n",
    "cls_targets = torch.zeros((batch_size, num_total_anchors), dtype=torch.long, device=device)\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "    # For positive anchors, assign the GT class label\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        gt_indices = matched_idxs[batch_idx][pos_mask]\n",
    "        target = targets[\"labels\"][batch_idx].to(device)\n",
    "        cls_targets[batch_idx, pos_mask] = target[gt_indices]\n",
    "\n",
    "cls_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_targets.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_targets = torch.zeros((batch_size, num_total_anchors), dtype=torch.float, device=device)\n",
    "for batch_idx in range(batch_size):\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        iou_targets[batch_idx, pos_mask] = max_ious[batch_idx][pos_mask]\n",
    "\n",
    "iou_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_targets = torch.zeros((batch_size, num_total_anchors, 4), dtype=torch.float, device=device)\n",
    "for batch_idx in range(batch_size):\n",
    "    pos_mask = matched_idxs[batch_idx] >= 0\n",
    "    if pos_mask.any():\n",
    "        gt_indices = matched_idxs[batch_idx][pos_mask]\n",
    "        bbox = targets[\"boxes\"][batch_idx].to(device)\n",
    "        \n",
    "        gt_boxes_matched = bbox[gt_indices]\n",
    "        reg_targets[batch_idx, pos_mask] = gt_boxes_matched\n",
    "\n",
    "reg_targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predictions(cls_outs, reg_outs, num_classes=10, num_bins=16):\n",
    "    batch_size = cls_outs[0].shape[0]\n",
    "    num_anchors = 6\n",
    "    \n",
    "    # Process each FPN level\n",
    "    all_cls_preds = []\n",
    "    all_reg_preds = []\n",
    "    \n",
    "    for cls_out, reg_out in zip(cls_outs, reg_outs):\n",
    "        # Get dimensions\n",
    "        B, C, H, W = cls_out.shape\n",
    "        \n",
    "        # For classification: [B, A*num_classes, H, W] -> [B, H*W*A, num_classes]\n",
    "        # First reshape to [B, A, num_classes, H, W]\n",
    "        reshaped_cls = cls_out.view(B, num_anchors, num_classes, H, W)\n",
    "        # Then permute to [B, H, W, A, num_classes]\n",
    "        permuted_cls = reshaped_cls.permute(0, 3, 4, 1, 2)\n",
    "        # Finally reshape to [B, H*W*A, num_classes]\n",
    "        flat_cls = permuted_cls.reshape(B, H*W*num_anchors, num_classes)\n",
    "        all_cls_preds.append(flat_cls)\n",
    "        \n",
    "        # For regression: [B, A*4*num_bins, H, W] -> [B, H*W*A, 4*num_bins]\n",
    "        # First reshape to [B, A, 4*num_bins, H, W]\n",
    "        reshaped_reg = reg_out.view(B, num_anchors, 4*num_bins, H, W)\n",
    "        # Then permute to [B, H, W, A, 4*num_bins]\n",
    "        permuted_reg = reshaped_reg.permute(0, 3, 4, 1, 2)\n",
    "        # Finally reshape to [B, H*W*A, 4*num_bins]\n",
    "        flat_reg = permuted_reg.reshape(B, H*W*num_anchors, 4*num_bins)\n",
    "        all_reg_preds.append(flat_reg)\n",
    "    \n",
    "    # Concatenate across FPN levels\n",
    "    cls_preds = torch.cat(all_cls_preds, dim=1)  # [B, N_total*A, num_classes]\n",
    "    reg_preds = torch.cat(all_reg_preds, dim=1)  # [B, N_total*A, 4*num_bins]\n",
    "    \n",
    "    return cls_preds, reg_preds\n",
    "\n",
    "cls_preds, reg_preds = prepare_predictions(cls_outs, reg_outs, num_classes=10, num_bins=16)\n",
    "cls_preds.size(), reg_preds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_focal_loss(pred, target, iou_targets, beta=2.0):\n",
    "    \"\"\"\n",
    "    Quality Focal Loss for dense object detection.\n",
    "    \n",
    "    Args:\n",
    "        pred: [B, N, C] logits tensor\n",
    "        target: [B, N] class indices tensor\n",
    "        iou_targets: [B, N] IoU values between anchors and GT boxes\n",
    "        beta: modulating factor\n",
    "    \n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    batch_size, num_anchors, num_classes = pred.shape\n",
    "    \n",
    "    # Create one-hot encoding for targets\n",
    "    target_one_hot = F.one_hot(target, num_classes).float()  # [B, N, C]\n",
    "    \n",
    "    # Sigmoid of predictions\n",
    "    pred_sigmoid = pred.sigmoid()\n",
    "    \n",
    "    # Get probability for the target class (pt)\n",
    "    pt = (target_one_hot * pred_sigmoid + (1 - target_one_hot) * (1 - pred_sigmoid))\n",
    "    \n",
    "    # Focal weight with IoU quality\n",
    "    # When IoU is high (high quality) and pt is low, we want a high weight\n",
    "    # When IoU is low (low quality) or pt is high, we want a low weight\n",
    "    weight = (iou_targets.unsqueeze(-1) * (1 - pt) + (1 - iou_targets.unsqueeze(-1)) * pt).pow(beta)\n",
    "    \n",
    "    # Binary cross entropy loss\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(\n",
    "        pred, target_one_hot, reduction='none'\n",
    "    )\n",
    "    \n",
    "    # Combine with weight\n",
    "    loss = bce_loss * weight\n",
    "    \n",
    "    # Determine positive samples for normalization\n",
    "    num_positive = (target > 0).sum().item()\n",
    "    \n",
    "    return loss.sum() / max(1, num_positive)\n",
    "\n",
    "quality_focal_loss(cls_preds, cls_targets, iou_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_focal_loss(pred, target, pos_mask=None):\n",
    "    \"\"\"\n",
    "    Distribution Focal Loss for bounding box regression.\n",
    "    \n",
    "    Args:\n",
    "        pred: [B, N, 4*bins] regression prediction\n",
    "        target: [B, N, 4] regression targets in [0, 1] range\n",
    "        pos_mask: [B, N] boolean mask for positive samples, or None to use all samples\n",
    "    \n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    # Extract dimensions\n",
    "    B, N, _ = pred.shape\n",
    "    num_bins = pred.shape[-1] // 4\n",
    "    \n",
    "    # Reshape predictions to [B, N, 4, bins]\n",
    "    pred = pred.reshape(B, N, 4, num_bins)\n",
    "    \n",
    "    # Convert target from continuous value to bin+offset\n",
    "    target_bins = (target * (num_bins - 1)).long()\n",
    "    target_bins = torch.clamp(target_bins, 0, num_bins - 2)\n",
    "    target_offset = (target * (num_bins - 1)) - target_bins.float()\n",
    "    \n",
    "    # Get left and right bin predictions\n",
    "    pred_left = torch.gather(pred, dim=3, index=target_bins.unsqueeze(-1)).squeeze(-1)\n",
    "    pred_right = torch.gather(pred, dim=3, index=(target_bins + 1).unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # DFL weighting\n",
    "    weight_left = 1 - target_offset\n",
    "    weight_right = target_offset\n",
    "    \n",
    "    # Alternative implementation using direct computation\n",
    "    # Reshape for BCELoss\n",
    "    pred_left = pred_left.reshape(-1)\n",
    "    pred_right = pred_right.reshape(-1)\n",
    "    weight_left = weight_left.reshape(-1)\n",
    "    weight_right = weight_right.reshape(-1)\n",
    "    \n",
    "    # Calculate losses\n",
    "    loss_left = F.binary_cross_entropy_with_logits(\n",
    "        pred_left, weight_left, reduction='none'\n",
    "    )\n",
    "    loss_right = F.binary_cross_entropy_with_logits(\n",
    "        pred_right, weight_right, reduction='none'\n",
    "    )\n",
    "    \n",
    "    # Reshape back\n",
    "    loss = (loss_left + loss_right).reshape(B, N, 4)\n",
    "    \n",
    "    # Apply positive mask if provided\n",
    "    if pos_mask is not None:\n",
    "        loss = loss * pos_mask.unsqueeze(-1)\n",
    "        num_positive = pos_mask.sum().item() * 4  # Multiply by 4 for coordinate dimension\n",
    "    else:\n",
    "        num_positive = B * N * 4  # Use all samples and all coordinates\n",
    "    \n",
    "    return loss.sum() / max(1, num_positive)\n",
    "\n",
    "pos_mask = matched_idxs >= 0  # [B, N]\n",
    "distribution_focal_loss(reg_preds, reg_targets, pos_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_dfl_bins(reg_preds, num_bins=16):\n",
    "    \"\"\"\n",
    "    Convert DFL output to continuous bounding box regression targets.\n",
    "\n",
    "    Args:\n",
    "        reg_preds: [B, N, 4 * num_bins]\n",
    "        num_bins: int, number of bins\n",
    "\n",
    "    Returns:\n",
    "        deltas: [B, N, 4] in (tx, ty, tw, th)\n",
    "    \"\"\"\n",
    "    B, N, _ = reg_preds.shape\n",
    "    reg_preds = reg_preds.view(B, N, 4, num_bins)  # [B, N, 4, bins]\n",
    "    prob = F.softmax(reg_preds, dim=-1)            # apply softmax over bins\n",
    "\n",
    "    bin_values = torch.arange(num_bins, dtype=torch.float32, device=reg_preds.device)  # [bins]\n",
    "    expected = (prob * bin_values).sum(dim=-1)  # [B, N, 4]\n",
    "\n",
    "    return expected / (num_bins - 1)  # normalize back to [0, 1] scale\n",
    "\n",
    "def delta2bbox(anchors, deltas):\n",
    "    \"\"\"\n",
    "    Decode regression deltas back to bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        anchors: [B, N, 4] in (x1, y1, x2, y2)\n",
    "        deltas:  [B, N, 4] in (tx, ty, tw, th)\n",
    "\n",
    "    Returns:\n",
    "        boxes: [B, N, 4] in (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    widths  = anchors[:, :, 2] - anchors[:, :, 0]\n",
    "    heights = anchors[:, :, 3] - anchors[:, :, 1]\n",
    "    ctr_x   = anchors[:, :, 0] + 0.5 * widths\n",
    "    ctr_y   = anchors[:, :, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, :, 0]\n",
    "    dy = deltas[:, :, 1]\n",
    "    dw = deltas[:, :, 2]\n",
    "    dh = deltas[:, :, 3]\n",
    "\n",
    "    pred_ctr_x = dx * widths + ctr_x\n",
    "    pred_ctr_y = dy * heights + ctr_y\n",
    "    pred_w = torch.exp(dw) * widths\n",
    "    pred_h = torch.exp(dh) * heights\n",
    "\n",
    "    x1 = pred_ctr_x - 0.5 * pred_w\n",
    "    y1 = pred_ctr_y - 0.5 * pred_h\n",
    "    x2 = pred_ctr_x + 0.5 * pred_w\n",
    "    y2 = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return torch.stack([x1, y1, x2, y2], dim=2)  # [B, N, 4]\n",
    "\n",
    "\n",
    "def giou_loss(pred_deltas, target_boxes, anchors, pos_mask):\n",
    "    B, N, _ = pred_deltas.shape\n",
    "    anchors = anchors.unsqueeze(0).expand(B, N, 4)  # [B, N, 4]\n",
    "    pred_boxes = delta2bbox(anchors, pred_deltas)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_pos = 0\n",
    "\n",
    "    for b in range(B):\n",
    "        pos = pos_mask[b]\n",
    "        if pos.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        pred_b = pred_boxes[b][pos]\n",
    "        target_b = target_boxes[b][pos]\n",
    "\n",
    "        giou = ops.generalized_box_iou(pred_b, target_b)\n",
    "        loss = 1.0 - giou.diagonal()  # only matched pairs\n",
    "\n",
    "        total_loss += loss.sum()\n",
    "        total_pos += len(loss)\n",
    "\n",
    "    return total_loss / max(total_pos, 1)\n",
    "\n",
    "\n",
    "# 1. Get pos_mask\n",
    "pos_mask = matched_idxs >= 0  # [B, N]\n",
    "\n",
    "# 2. Decode DFL bins to deltas\n",
    "reg_deltas = decode_dfl_bins(reg_preds, num_bins=16)  # [B, N, 4]\n",
    "\n",
    "# 3. Prepare anchor tensor\n",
    "all_anchors = torch.cat(anchors, dim=0).to(device)  # [N, 4]\n",
    "\n",
    "# 4. Compute GIoU Loss\n",
    "giou = giou_loss(reg_deltas, reg_targets, all_anchors, pos_mask)\n",
    "giou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models.losses import QualityFocalLoss   # or use your custom one\n",
    "\n",
    "qfl = QualityFocalLoss(use_sigmoid=True, beta=2.0, reduction='mean', loss_weight=1.0)\n",
    "\n",
    "# Example for a single image (i-th in batch)\n",
    "i = 0\n",
    "# Filter positive anchors\n",
    "pos_mask = matched_idxs[i] >= 0\n",
    "pos_inds = pos_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Prepare predictions and targets for QFL\n",
    "qfl_pred = cls_preds[i][pos_inds]             # [num_pos, num_classes]\n",
    "qfl_labels = cls_targets[i][pos_inds]         # [num_pos] class labels (long)\n",
    "qfl_scores = iou_targets[i][pos_inds]         # [num_pos] IoU scores (float)\n",
    "\n",
    "# Format target as a tuple for QFL\n",
    "qfl_target = (qfl_labels, qfl_scores)\n",
    "\n",
    "loss = qfl(qfl_pred, qfl_target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models.losses import DistributionFocalLoss\n",
    "class GFLBBoxCoder:\n",
    "    def __init__(self, num_bins=16):\n",
    "        self.num_bins = num_bins\n",
    "\n",
    "    def encode(self, anchors, gt_boxes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            anchors: [N, 4] in (x1, y1, x2, y2)\n",
    "            gt_boxes: [N, 4] in (x1, y1, x2, y2)\n",
    "        Returns:\n",
    "            reg_targets: [N, 4] float values in [0, num_bins)\n",
    "        \"\"\"\n",
    "        # convert to ltrb format\n",
    "        anchor_w = anchors[:, 2] - anchors[:, 0]\n",
    "        anchor_h = anchors[:, 3] - anchors[:, 1]\n",
    "\n",
    "        t = (gt_boxes[:, 1] - anchors[:, 1]) / anchor_h * (self.num_bins - 1)\n",
    "        l = (gt_boxes[:, 0] - anchors[:, 0]) / anchor_w * (self.num_bins - 1)\n",
    "        b = (gt_boxes[:, 3] - anchors[:, 3]) / anchor_h * (self.num_bins - 1)\n",
    "        r = (gt_boxes[:, 2] - anchors[:, 2]) / anchor_w * (self.num_bins - 1)\n",
    "\n",
    "        reg_targets = torch.stack([l, t, r, b], dim=-1).clamp(0, self.num_bins - 1)\n",
    "        return reg_targets\n",
    "\n",
    "i = 0\n",
    "pos_mask = matched_idxs[i] >= 0\n",
    "pos_inds = pos_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "# Ensure all_anchors is a tensor\n",
    "if not isinstance(all_anchors, torch.Tensor):\n",
    "    all_anchors = torch.tensor(all_anchors).to(reg_preds.device).float()\n",
    "\n",
    "reg_pred = reg_preds[i][pos_inds]           # [num_pos, 64]\n",
    "pos_anchors = all_anchors[pos_inds]         # [num_pos, 4]\n",
    "pos_gt_boxes = reg_targets[i][matched_idxs[i][pos_inds]]  # [num_pos, 4]\n",
    "\n",
    "bbox_coder = GFLBBoxCoder(num_bins=16)\n",
    "encoded_reg_target = bbox_coder.encode(pos_anchors, pos_gt_boxes)  # [num_pos, 4]\n",
    "\n",
    "# Flatten for DFL\n",
    "reg_pred = reg_pred.view(-1, 16)            # [num_pos * 4, 16]\n",
    "reg_target = encoded_reg_target.view(-1)    # [num_pos * 4]\n",
    "\n",
    "dfl = DistributionFocalLoss(reduction='mean', loss_weight=1.0)\n",
    "loss = dfl(reg_pred, reg_target)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.models.losses import GIoULoss\n",
    "\n",
    "def decode_dfl_bbox(reg_pred_logits, anchors, num_bins=16):\n",
    "    B, _ = reg_pred_logits.shape\n",
    "    reg_pred_logits = reg_pred_logits.view(B, 4, num_bins)\n",
    "    prob = torch.softmax(reg_pred_logits, dim=2)\n",
    "    bins = torch.arange(num_bins, dtype=prob.dtype, device=prob.device)\n",
    "    dist = torch.sum(prob * bins, dim=2) / (num_bins - 1)\n",
    "\n",
    "    anchor_w = anchors[:, 2] - anchors[:, 0]\n",
    "    anchor_h = anchors[:, 3] - anchors[:, 1]\n",
    "\n",
    "    l = dist[:, 0] * anchor_w\n",
    "    t = dist[:, 1] * anchor_h\n",
    "    r = dist[:, 2] * anchor_w\n",
    "    b = dist[:, 3] * anchor_h\n",
    "\n",
    "    x1 = anchors[:, 0] - l\n",
    "    y1 = anchors[:, 1] - t\n",
    "    x2 = anchors[:, 2] + r\n",
    "    y2 = anchors[:, 3] + b\n",
    "\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "# === Setup ===\n",
    "i = 0\n",
    "pos_mask = matched_idxs[i] >= 0\n",
    "pos_inds = pos_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "if not isinstance(all_anchors, torch.Tensor):\n",
    "    all_anchors = torch.tensor(all_anchors).to(reg_preds.device).float()\n",
    "\n",
    "reg_pred_logits = reg_preds[i][pos_inds]\n",
    "anchors_pos = all_anchors[pos_inds]\n",
    "\n",
    "pred_boxes = decode_dfl_bbox(reg_pred_logits, anchors_pos, num_bins=16)\n",
    "target_boxes = reg_targets[i][matched_idxs[i][pos_inds]]\n",
    "\n",
    "giou_loss_fn = GIoULoss(reduction='mean', loss_weight=1.0)\n",
    "loss = giou_loss_fn(pred_boxes, target_boxes)\n",
    "\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
