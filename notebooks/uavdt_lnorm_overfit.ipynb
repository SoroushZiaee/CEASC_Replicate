{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767763-752c-4c9a-8119-4e312c5204ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for overfitting Lnorm on one image of UAVDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d36f49-6812-49bd-bf78-573d95418b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add parent directory, it should add parent of parent\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.uavdt_dataloader import get_dataset\n",
    "from utils.losses import Lnorm, Lamm  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a28dba7-0b10-46fa-bf73-e9b20e867bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3596626a-595d-42ab-b80f-32471030ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the setup \n",
    "mode = \"train\"  # Change to \"eval\" or \"test\" as needed\n",
    "\n",
    "config = {\n",
    "    \"root_dir\": \"/home/eyakub/scratch/CEASC_replicate\",\n",
    "    \"batch_size\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1c0ed48-ae61-45c5-af48-465bcaa6a259",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/eyakub/scratch/CEASC_replicate/UAV-benchmark-M/trainset_meta.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Dataset and loader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m dataloader = \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[32m     22\u001b[39m model = Res18FPNCEASC(config_path=config[\u001b[33m\"\u001b[39m\u001b[33mconfig_path\u001b[39m\u001b[33m\"\u001b[39m], num_classes=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre06/project/6067616/soroush1/CEASC_Replicate/notebooks/../utils/uavdt_dataloader.py:36\u001b[39m, in \u001b[36mget_dataset\u001b[39m\u001b[34m(root_dir, split, transform, batch_size, shuffle, num_workers)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03mGet the dataset based on the split.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m:param root_dir: Root directory of the dataset.\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m:param split: Split of the dataset (train, val, test).\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m:return: Dataloader object.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m transform = transformation(split) \u001b[38;5;28;01mif\u001b[39;00m transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m transform\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m dataset = \u001b[43mUAVDTDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m dataloader = DataLoader(\n\u001b[32m     39\u001b[39m     dataset,\n\u001b[32m     40\u001b[39m     batch_size=batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     collate_fn=detection_collate,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/lustre06/project/6067616/soroush1/CEASC_Replicate/notebooks/../data/uavdt_dataset.py:24\u001b[39m, in \u001b[36mUAVDTDataset.__init__\u001b[39m\u001b[34m(self, root_dir, split, transform)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.categories = {\n\u001b[32m     17\u001b[39m     \u001b[32m0\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcar\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[32m1\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtruck\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[32m2\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbus\u001b[39m\u001b[33m\"\u001b[39m \n\u001b[32m     20\u001b[39m }\n\u001b[32m     22\u001b[39m meta_path = os.path.join(\u001b[38;5;28mself\u001b[39m.root_dir, \u001b[33m\"\u001b[39m\u001b[33mUAV-benchmark-M\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mset_meta.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmeta_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m meta_file:\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mself\u001b[39m.imgs = meta_file.readlines() \u001b[38;5;66;03m# get the list of imgs from the csv file \u001b[39;00m\n\u001b[32m     26\u001b[39m meta_file.close()\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '/home/eyakub/scratch/CEASC_replicate/UAV-benchmark-M/trainset_meta.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Unpack config\n",
    "    root_dir = config[\"root_dir\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    learning_rate = config[\"lr\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataloader = get_dataset(\n",
    "        root_dir=root_dir,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=3)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "    \n",
    "    # Losses\n",
    "    l_norm = Lnorm()\n",
    "\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    images = batch[\"image\"].to(device)\n",
    "    targets = {\n",
    "        \"boxes\": batch[\"boxes\"],\n",
    "        \"labels\": batch[\"labels\"],\n",
    "        \"image_id\": batch[\"image_id\"],\n",
    "        \"orig_size\": batch[\"orig_size\"],\n",
    "    }\n",
    "    print(\"\\nüîç Inspecting `targets` structure:\")\n",
    "    for i in range(len(targets[\"boxes\"])):\n",
    "        print(f\"--- Sample {i} ---\")\n",
    "        print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "        print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "        print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "        print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "        print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "        print(f\"Labels:           {targets['labels'][i]}\")\n",
    "\n",
    "    n_iters = 2000\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for n in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images, stage=\"train\")\n",
    "        (\n",
    "            cls_outs,\n",
    "            reg_outs,\n",
    "            soft_mask_outs,\n",
    "            sparse_cls_feats_outs,\n",
    "            sparse_reg_feats_outs,\n",
    "            dense_cls_feats_outs,\n",
    "            dense_reg_feats_outs,\n",
    "            feats,\n",
    "            anchors,\n",
    "        ) = outputs\n",
    "\n",
    "        # print(\"\\nüîç Output shapes from model:\")\n",
    "        # for i in range(len(cls_outs)):\n",
    "        #     print(f\"--- FPN Level {i} ---\")\n",
    "        #     print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\")\n",
    "        #     print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\")\n",
    "        #     print(\n",
    "        #         f\"soft_mask_outs[{i}]:    {safe_shape(soft_mask_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "        \n",
    "        # for i, anchor in enumerate(anchors):\n",
    "        #     print(f\"P{i+3} Anchors shape: {anchor.shape}\")\n",
    "\n",
    "        loss_norm = l_norm(\n",
    "                sparse_cls_feats_outs, soft_mask_outs, dense_cls_feats_outs\n",
    "            )\n",
    "    \n",
    "        if n % 100 == 0:\n",
    "            print(f\"Loss Norm, iter {n}: {loss_norm.item()}\")\n",
    "\n",
    "        writer.add_scalar('Norm Loss/overfit',loss_norm.item(),n)\n",
    "\n",
    "        loss_norm.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    print('Overfit complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15553460-9e37-4c1f-955e-a084e19defde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
