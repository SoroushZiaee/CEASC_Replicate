{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806fa3fd-29d1-4f75-8e36-c7100b1959e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for overfitting Lnorm on one image of UAVDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e70a5d6-e731-405b-bfe5-fba0968f0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# add parent directory, it should add parent of parent\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.dataset import get_dataset\n",
    "from utils.losses import Lnorm, Lamm  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73024509-5640-4aae-b8c2-bb2292bb249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc98f2c-0835-41be-9bb1-5e57d9e5fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the setup \n",
    "mode = \"train\"  # Change to \"eval\" or \"test\" as needed\n",
    "\n",
    "config = {\n",
    "    \"root_dir\": \"/home/eyakub/scratch/CEASC_replicate\",\n",
    "    \"batch_size\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868a5894-f75c-427d-b1d6-d10125bab770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Inspecting `targets` structure:\n",
      "--- Sample 0 ---\n",
      "Image ID:         tensor([790])\n",
      "Original Size:    tensor([ 765, 1360])\n",
      "Boxes shape:      torch.Size([10, 4])\n",
      "Labels shape:     torch.Size([10])\n",
      "Boxes:            tensor([[ 646.,  303.,  703.,  327.],\n",
      "        [ 851.,  314.,  916.,  338.],\n",
      "        [1158.,  327., 1221.,  353.],\n",
      "        [ 641.,  178.,  661.,  194.],\n",
      "        [ 646.,  237.,  655.,  245.],\n",
      "        [ 780.,  188.,  819.,  209.],\n",
      "        [ 814.,  206.,  829.,  237.],\n",
      "        [ 725.,  248.,  737.,  260.],\n",
      "        [1003.,  221., 1012.,  234.],\n",
      "        [ 872.,  377.,  916.,  434.]])\n",
      "Labels:           tensor([4, 4, 4, 4, 4, 6, 6, 4, 4, 6])\n",
      "Loss Norm, iter 0: 0.8996548652648926\n",
      "Loss Norm, iter 100: 0.27682024240493774\n",
      "Loss Norm, iter 200: 0.21848975121974945\n",
      "Loss Norm, iter 300: 0.19001396000385284\n",
      "Loss Norm, iter 400: 0.17312325537204742\n",
      "Loss Norm, iter 500: 0.16176146268844604\n",
      "Loss Norm, iter 600: 0.153717502951622\n",
      "Loss Norm, iter 700: 0.14761511981487274\n",
      "Loss Norm, iter 800: 0.14267568290233612\n",
      "Loss Norm, iter 900: 0.13933974504470825\n",
      "Loss Norm, iter 1000: 0.1363597959280014\n",
      "Loss Norm, iter 1100: 0.13393442332744598\n",
      "Loss Norm, iter 1200: 0.13198786973953247\n",
      "Loss Norm, iter 1300: 0.13037510216236115\n",
      "Loss Norm, iter 1400: 0.12898100912570953\n",
      "Loss Norm, iter 1500: 0.12784896790981293\n",
      "Loss Norm, iter 1600: 0.12688221037387848\n",
      "Loss Norm, iter 1700: 0.126042440533638\n",
      "Loss Norm, iter 1800: 0.1252894103527069\n",
      "Loss Norm, iter 1900: 0.12460281699895859\n",
      "Overfit complete\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Unpack config\n",
    "    root_dir = config[\"root_dir\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    learning_rate = config[\"lr\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataloader = get_dataset(\n",
    "        root_dir=root_dir,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=10)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "    \n",
    "    # Losses\n",
    "    l_norm = Lnorm()\n",
    "\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    images = batch[\"image\"].to(device)\n",
    "    targets = {\n",
    "        \"boxes\": batch[\"boxes\"],\n",
    "        \"labels\": batch[\"labels\"],\n",
    "        \"image_id\": batch[\"image_id\"],\n",
    "        \"orig_size\": batch[\"orig_size\"],\n",
    "    }\n",
    "    print(\"\\nüîç Inspecting `targets` structure:\")\n",
    "    for i in range(len(targets[\"boxes\"])):\n",
    "        print(f\"--- Sample {i} ---\")\n",
    "        print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "        print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "        print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "        print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "        print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "        print(f\"Labels:           {targets['labels'][i]}\")\n",
    "\n",
    "    n_iters = 2000\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for n in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images, stage=\"train\")\n",
    "        (\n",
    "            cls_outs,\n",
    "            reg_outs,\n",
    "            soft_mask_outs,\n",
    "            sparse_cls_feats_outs,\n",
    "            sparse_reg_feats_outs,\n",
    "            dense_cls_feats_outs,\n",
    "            dense_reg_feats_outs,\n",
    "            feats,\n",
    "            anchors,\n",
    "        ) = outputs\n",
    "\n",
    "        # print(\"\\nüîç Output shapes from model:\")\n",
    "        # for i in range(len(cls_outs)):\n",
    "        #     print(f\"--- FPN Level {i} ---\")\n",
    "        #     print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\")\n",
    "        #     print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\")\n",
    "        #     print(\n",
    "        #         f\"soft_mask_outs[{i}]:    {safe_shape(soft_mask_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "        \n",
    "        # for i, anchor in enumerate(anchors):\n",
    "        #     print(f\"P{i+3} Anchors shape: {anchor.shape}\")\n",
    "\n",
    "        loss_norm = l_norm(\n",
    "                sparse_cls_feats_outs, soft_mask_outs, dense_cls_feats_outs\n",
    "            )\n",
    "    \n",
    "        if n % 100 == 0:\n",
    "            print(f\"Loss Norm, iter {n}: {loss_norm.item()}\")\n",
    "\n",
    "        writer.add_scalar('Norm Loss/overfit',loss_norm.item(),n)\n",
    "\n",
    "        loss_norm.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    print('Overfit complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6575b7a-ed88-4c7e-be23-1f3d5fd3e127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
