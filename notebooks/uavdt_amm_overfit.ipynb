{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6ffa8-7e7a-430e-a0a0-8f90b7b883d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook for overfitting AMM loss on one image from UAVDT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5fb4dd-e43f-49fc-bdc1-7db7fd3d28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# add parent directory, it should add parent of parent\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.uavdt_dataloader import get_dataset\n",
    "from utils.losses import Lnorm, Lamm  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4beb682-a5f9-4bb1-9f77-b8034ad626f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14472aa-1612-489c-8931-7563004566de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/lustre06/project/6067616/eyakub/CEASC_Replicate/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81f779e-2124-4dd1-b267-8fb64567efa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the setup \n",
    "mode = \"train\"  # Change to \"eval\" or \"test\" as needed\n",
    "\n",
    "config = {\n",
    "    \"root_dir\": \"/home/eyakub/scratch/CEASC_replicate\",\n",
    "    \"batch_size\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770e1349-d88b-4b37-8901-ae28be34cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamm init called\n",
      "\n",
      "üîç Inspecting `targets` structure:\n",
      "--- Sample 0 ---\n",
      "Image ID:         tensor([1386])\n",
      "Original Size:    tensor([ 540, 1024])\n",
      "Boxes shape:      torch.Size([8, 4])\n",
      "Labels shape:     torch.Size([8])\n",
      "Boxes:            tensor([[ 735.,   66.,  749.,   82.],\n",
      "        [ 750.,  253.,  771.,  280.],\n",
      "        [ 746.,  324.,  769.,  356.],\n",
      "        [ 734.,  420.,  763.,  465.],\n",
      "        [ 903.,  153.,  936.,  174.],\n",
      "        [ 685.,  131.,  702.,  150.],\n",
      "        [ 989.,  198., 1022.,  222.],\n",
      "        [ 786.,   33.,  799.,   46.]])\n",
      "Labels:           tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Loss AMM, iter 0: 0.24558420479297638\n",
      "Loss AMM, iter 1: 0.24329867959022522\n",
      "Loss AMM, iter 2: 0.23299837112426758\n",
      "Loss AMM, iter 3: 0.22372357547283173\n",
      "Loss AMM, iter 4: 0.21702896058559418\n",
      "Loss AMM, iter 5: 0.2070760279893875\n",
      "Loss AMM, iter 6: 0.21523742377758026\n",
      "Loss AMM, iter 7: 0.211978480219841\n",
      "Loss AMM, iter 8: 0.204514741897583\n",
      "Loss AMM, iter 9: 0.19740013778209686\n",
      "Loss AMM, iter 10: 0.19644497334957123\n",
      "Loss AMM, iter 11: 0.19613201916217804\n",
      "Loss AMM, iter 12: 0.19661851227283478\n",
      "Loss AMM, iter 13: 0.1857852190732956\n",
      "Loss AMM, iter 14: 0.1877042055130005\n",
      "Loss AMM, iter 15: 0.1748025119304657\n",
      "Loss AMM, iter 16: 0.16822877526283264\n",
      "Loss AMM, iter 17: 0.17292442917823792\n",
      "Loss AMM, iter 18: 0.16416141390800476\n",
      "Loss AMM, iter 19: 0.15151265263557434\n",
      "Loss AMM, iter 20: 0.16150659322738647\n",
      "Loss AMM, iter 21: 0.14761821925640106\n",
      "Loss AMM, iter 22: 0.14274905622005463\n",
      "Loss AMM, iter 23: 0.13462689518928528\n",
      "Loss AMM, iter 24: 0.13801831007003784\n",
      "Loss AMM, iter 25: 0.13735434412956238\n",
      "Loss AMM, iter 26: 0.1266983151435852\n",
      "Loss AMM, iter 27: 0.1261020302772522\n",
      "Loss AMM, iter 28: 0.11517932265996933\n",
      "Loss AMM, iter 29: 0.1174660250544548\n",
      "Loss AMM, iter 30: 0.11207538098096848\n",
      "Loss AMM, iter 31: 0.11536306142807007\n",
      "Loss AMM, iter 32: 0.11141221970319748\n",
      "Loss AMM, iter 33: 0.10748273134231567\n",
      "Loss AMM, iter 34: 0.0976712629199028\n",
      "Loss AMM, iter 35: 0.09402437508106232\n",
      "Loss AMM, iter 36: 0.08912201970815659\n",
      "Loss AMM, iter 37: 0.08747058361768723\n",
      "Loss AMM, iter 38: 0.08605896681547165\n",
      "Loss AMM, iter 39: 0.07672182470560074\n",
      "Loss AMM, iter 40: 0.08058018982410431\n",
      "Loss AMM, iter 41: 0.07140358537435532\n",
      "Loss AMM, iter 42: 0.0716453567147255\n",
      "Loss AMM, iter 43: 0.0654149278998375\n",
      "Loss AMM, iter 44: 0.064661405980587\n",
      "Loss AMM, iter 45: 0.06038187816739082\n",
      "Loss AMM, iter 46: 0.053857650607824326\n",
      "Loss AMM, iter 47: 0.05380173400044441\n",
      "Loss AMM, iter 48: 0.0430050827562809\n",
      "Loss AMM, iter 49: 0.05432795360684395\n",
      "Loss AMM, iter 50: 0.0405176617205143\n",
      "Loss AMM, iter 51: 0.037249352782964706\n",
      "Loss AMM, iter 52: 0.040057796984910965\n",
      "Loss AMM, iter 53: 0.03238421678543091\n",
      "Loss AMM, iter 54: 0.03702666983008385\n",
      "Loss AMM, iter 55: 0.03494342043995857\n",
      "Loss AMM, iter 56: 0.03309832885861397\n",
      "Loss AMM, iter 57: 0.024045992642641068\n",
      "Loss AMM, iter 58: 0.0245454590767622\n",
      "Loss AMM, iter 59: 0.023936355486512184\n",
      "Loss AMM, iter 60: 0.025770390406250954\n",
      "Loss AMM, iter 61: 0.020859098061919212\n",
      "Loss AMM, iter 62: 0.018788183107972145\n",
      "Loss AMM, iter 63: 0.017372578382492065\n",
      "Loss AMM, iter 64: 0.01670023798942566\n",
      "Loss AMM, iter 65: 0.01447539683431387\n",
      "Loss AMM, iter 66: 0.015927154570817947\n",
      "Loss AMM, iter 67: 0.017027100548148155\n",
      "Loss AMM, iter 68: 0.01521649956703186\n",
      "Loss AMM, iter 69: 0.014639697037637234\n",
      "Loss AMM, iter 70: 0.014379597268998623\n",
      "Loss AMM, iter 71: 0.010543596930801868\n",
      "Loss AMM, iter 72: 0.00964092742651701\n",
      "Loss AMM, iter 73: 0.009708758443593979\n",
      "Loss AMM, iter 74: 0.010056513361632824\n",
      "Loss AMM, iter 75: 0.008517354726791382\n",
      "Loss AMM, iter 76: 0.007619890850037336\n",
      "Loss AMM, iter 77: 0.008512859232723713\n",
      "Loss AMM, iter 78: 0.0062298583798110485\n",
      "Loss AMM, iter 79: 0.006159414537250996\n",
      "Loss AMM, iter 80: 0.008162078447639942\n",
      "Loss AMM, iter 81: 0.006549079902470112\n",
      "Loss AMM, iter 82: 0.006626022048294544\n",
      "Loss AMM, iter 83: 0.006871229503303766\n",
      "Loss AMM, iter 84: 0.006662799511104822\n",
      "Loss AMM, iter 85: 0.005436910316348076\n",
      "Loss AMM, iter 86: 0.005721861030906439\n",
      "Loss AMM, iter 87: 0.005210918374359608\n",
      "Loss AMM, iter 88: 0.005667468067258596\n",
      "Loss AMM, iter 89: 0.005517063196748495\n",
      "Loss AMM, iter 90: 0.005378998350352049\n",
      "Loss AMM, iter 91: 0.0052627758122980595\n",
      "Loss AMM, iter 92: 0.00603502755984664\n",
      "Loss AMM, iter 93: 0.0054145315662026405\n",
      "Loss AMM, iter 94: 0.003457607701420784\n",
      "Loss AMM, iter 95: 0.004388807807117701\n",
      "Loss AMM, iter 96: 0.005076136440038681\n",
      "Loss AMM, iter 97: 0.0041564712300896645\n",
      "Loss AMM, iter 98: 0.0031632327008992434\n",
      "Loss AMM, iter 99: 0.003904143115505576\n",
      "Overfit complete\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Get the dictionary for feature visualization\n",
    "    vis_dict = {}\n",
    "    \n",
    "    # Unpack config\n",
    "    root_dir = config[\"root_dir\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    learning_rate = config[\"lr\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataloader = get_dataset(\n",
    "        root_dir=root_dir,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=3)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "    \n",
    "    # Losses\n",
    "    l_amm = Lamm()\n",
    "\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    images = batch[\"image\"].to(device)\n",
    "    targets = {\n",
    "        \"boxes\": batch[\"boxes\"],\n",
    "        \"labels\": batch[\"labels\"],\n",
    "        \"image_id\": batch[\"image_id\"],\n",
    "        \"orig_size\": batch[\"orig_size\"],\n",
    "    }\n",
    "    print(\"\\nüîç Inspecting `targets` structure:\")\n",
    "    for i in range(len(targets[\"boxes\"])):\n",
    "        print(f\"--- Sample {i} ---\")\n",
    "        print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "        print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "        print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "        print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "        print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "        print(f\"Labels:           {targets['labels'][i]}\")\n",
    "\n",
    "    vis_dict[\"image_id\"] = targets[\"image_id\"]\n",
    "    \n",
    "    n_iters = 100\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for n in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images, stage=\"train\")\n",
    "        (\n",
    "            cls_outs,\n",
    "            reg_outs,\n",
    "            soft_mask_outs,\n",
    "            sparse_cls_feats_outs,\n",
    "            sparse_reg_feats_outs,\n",
    "            dense_cls_feats_outs,\n",
    "            dense_reg_feats_outs,\n",
    "            feats,\n",
    "            anchors,\n",
    "        ) = outputs\n",
    "\n",
    "        if n == 24 or n == 49 or n ==99:\n",
    "            vis_dict[f\"{n}_mask\"] = [s.clone().detach() for s in soft_mask_outs]\n",
    "\n",
    "        # print(\"\\nüîç Output shapes from model:\")\n",
    "        # for i in range(len(cls_outs)):\n",
    "        #     print(f\"--- FPN Level {i} ---\")\n",
    "        #     print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\")\n",
    "        #     print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\")\n",
    "        #     print(\n",
    "        #         f\"soft_mask_outs[{i}]:    {safe_shape(soft_mask_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "        \n",
    "        # for i, anchor in enumerate(anchors):\n",
    "        #     print(f\"P{i+3} Anchors shape: {anchor.shape}\")\n",
    "\n",
    "        loss_amm = l_amm(\n",
    "            soft_mask_outs, targets[\"boxes\"], im_dimx=1024, im_dimy=540\n",
    "        )  \n",
    "    \n",
    "        print(f\"Loss AMM, iter {n}: {loss_amm.item()}\")\n",
    "\n",
    "        writer.add_scalar('AMM Loss/overfit',loss_amm.item(),n)\n",
    "\n",
    "        loss_amm.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    print('Overfit complete')\n",
    "\n",
    "    with open(\"uavdt_masks.pickle\",\"wb\") as f:\n",
    "        pickle.dump(vis_dict,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13db43ba-3902-4b77-9acb-672bc04acfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
