{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efefec4d-a5dc-4500-9478-da32459c6b98",
   "metadata": {},
   "source": [
    "# notebook for overfitting AMM loss on one image from VisDrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26f49e41-7878-4e86-9baf-30cd47174c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae54e78-80bb-4398-9336-09a5484f36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# add parent directory, it should add parent of parent\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Res18FPNCEASC  # Adjust as needed\n",
    "from utils.visdrone_dataloader import get_dataset\n",
    "from utils.losses import Lnorm, Lamm  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e0634d-0e94-45e5-8f14-9f9806750ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_shape(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.shape\n",
    "    elif isinstance(x, (list, tuple)):\n",
    "        return [safe_shape(e) for e in x]\n",
    "    return type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5371555-34f2-450a-a899-5f57e454664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the setup \n",
    "mode = \"train\"  # Change to \"eval\" or \"test\" as needed\n",
    "\n",
    "config = {\n",
    "    \"root_dir\": \"/home/soroush1/scratch/eecs_project\",\n",
    "    \"batch_size\": 1,\n",
    "    \"num_workers\": 4,\n",
    "    \"num_epochs\": 1,\n",
    "    \"lr\": 1e-2,\n",
    "    \"config_path\": \"../configs/resnet18_fpn_feature_extractor.py\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99307604-7b1e-490c-8a09-dfb4a189b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamm init called\n",
      "Loss AMM, iter 0: 0.2353719025850296\n",
      "Loss AMM, iter 1: 0.23240871727466583\n",
      "Loss AMM, iter 2: 0.22862663865089417\n",
      "Loss AMM, iter 3: 0.23308204114437103\n",
      "Loss AMM, iter 4: 0.23013949394226074\n",
      "Loss AMM, iter 5: 0.222884401679039\n",
      "Loss AMM, iter 6: 0.21268267929553986\n",
      "Loss AMM, iter 7: 0.21511700749397278\n",
      "Loss AMM, iter 8: 0.20486898720264435\n",
      "Loss AMM, iter 9: 0.20724628865718842\n",
      "Loss AMM, iter 10: 0.1933915913105011\n",
      "Loss AMM, iter 11: 0.19243361055850983\n",
      "Loss AMM, iter 12: 0.1890181005001068\n",
      "Loss AMM, iter 13: 0.1800224781036377\n",
      "Loss AMM, iter 14: 0.18674185872077942\n",
      "Loss AMM, iter 15: 0.18616944551467896\n",
      "Loss AMM, iter 16: 0.1758638173341751\n",
      "Loss AMM, iter 17: 0.17470967769622803\n",
      "Loss AMM, iter 18: 0.1687769889831543\n",
      "Loss AMM, iter 19: 0.16449229419231415\n",
      "Loss AMM, iter 20: 0.164544939994812\n",
      "Loss AMM, iter 21: 0.16062717139720917\n",
      "Loss AMM, iter 22: 0.14974288642406464\n",
      "Loss AMM, iter 23: 0.15504346787929535\n",
      "Loss AMM, iter 24: 0.14641724526882172\n",
      "Loss AMM, iter 25: 0.1478549689054489\n",
      "Loss AMM, iter 26: 0.14780990779399872\n",
      "Loss AMM, iter 27: 0.13344745337963104\n",
      "Loss AMM, iter 28: 0.13473089039325714\n",
      "Loss AMM, iter 29: 0.13070982694625854\n",
      "Loss AMM, iter 30: 0.1279047578573227\n",
      "Loss AMM, iter 31: 0.12759080529212952\n",
      "Loss AMM, iter 32: 0.11869888752698898\n",
      "Loss AMM, iter 33: 0.12463026493787766\n",
      "Loss AMM, iter 34: 0.11437533050775528\n",
      "Loss AMM, iter 35: 0.11133068799972534\n",
      "Loss AMM, iter 36: 0.10340564697980881\n",
      "Loss AMM, iter 37: 0.10664528608322144\n",
      "Loss AMM, iter 38: 0.09629165381193161\n",
      "Loss AMM, iter 39: 0.09767742455005646\n",
      "Loss AMM, iter 40: 0.09055639803409576\n",
      "Loss AMM, iter 41: 0.08192485570907593\n",
      "Loss AMM, iter 42: 0.08201195299625397\n",
      "Loss AMM, iter 43: 0.07514691352844238\n",
      "Loss AMM, iter 44: 0.07577209919691086\n",
      "Loss AMM, iter 45: 0.0670933946967125\n",
      "Loss AMM, iter 46: 0.0679873526096344\n",
      "Loss AMM, iter 47: 0.06118055805563927\n",
      "Loss AMM, iter 48: 0.05616411566734314\n",
      "Loss AMM, iter 49: 0.058658041059970856\n",
      "Loss AMM, iter 50: 0.05276268720626831\n",
      "Loss AMM, iter 51: 0.05383149906992912\n",
      "Loss AMM, iter 52: 0.05232517793774605\n",
      "Loss AMM, iter 53: 0.04650600627064705\n",
      "Loss AMM, iter 54: 0.04326809197664261\n",
      "Loss AMM, iter 55: 0.041997868567705154\n",
      "Loss AMM, iter 56: 0.04021108150482178\n",
      "Loss AMM, iter 57: 0.038361769169569016\n",
      "Loss AMM, iter 58: 0.03360601142048836\n",
      "Loss AMM, iter 59: 0.03334438428282738\n",
      "Loss AMM, iter 60: 0.030844945460557938\n",
      "Loss AMM, iter 61: 0.028962746262550354\n",
      "Loss AMM, iter 62: 0.028359627351164818\n",
      "Loss AMM, iter 63: 0.025682825595140457\n",
      "Loss AMM, iter 64: 0.026996755972504616\n",
      "Loss AMM, iter 65: 0.0226524006575346\n",
      "Loss AMM, iter 66: 0.02234056033194065\n",
      "Loss AMM, iter 67: 0.02275208942592144\n",
      "Loss AMM, iter 68: 0.018765587359666824\n",
      "Loss AMM, iter 69: 0.020740104839205742\n",
      "Loss AMM, iter 70: 0.017980817705392838\n",
      "Loss AMM, iter 71: 0.017706265673041344\n",
      "Loss AMM, iter 72: 0.01622006669640541\n",
      "Loss AMM, iter 73: 0.015105107799172401\n",
      "Loss AMM, iter 74: 0.014426183886826038\n",
      "Loss AMM, iter 75: 0.013819134794175625\n",
      "Loss AMM, iter 76: 0.014279097318649292\n",
      "Loss AMM, iter 77: 0.015239635482430458\n",
      "Loss AMM, iter 78: 0.014144358225166798\n",
      "Loss AMM, iter 79: 0.012656903825700283\n",
      "Loss AMM, iter 80: 0.011060305871069431\n",
      "Loss AMM, iter 81: 0.010633130557835102\n",
      "Loss AMM, iter 82: 0.01158637274056673\n",
      "Loss AMM, iter 83: 0.010431637056171894\n",
      "Loss AMM, iter 84: 0.01082121767103672\n",
      "Loss AMM, iter 85: 0.010480842553079128\n",
      "Loss AMM, iter 86: 0.00968901813030243\n",
      "Loss AMM, iter 87: 0.009046007879078388\n",
      "Loss AMM, iter 88: 0.008560676127672195\n",
      "Loss AMM, iter 89: 0.008275819011032581\n",
      "Loss AMM, iter 90: 0.008776065893471241\n",
      "Loss AMM, iter 91: 0.008386357687413692\n",
      "Loss AMM, iter 92: 0.007646888494491577\n",
      "Loss AMM, iter 93: 0.007013515569269657\n",
      "Loss AMM, iter 94: 0.006762571632862091\n",
      "Loss AMM, iter 95: 0.007453848607838154\n",
      "Loss AMM, iter 96: 0.006196106784045696\n",
      "Loss AMM, iter 97: 0.006743222009390593\n",
      "Loss AMM, iter 98: 0.006495690438896418\n",
      "Loss AMM, iter 99: 0.00558753777295351\n",
      "Loss AMM, iter 100: 0.005947708617895842\n",
      "Loss AMM, iter 101: 0.005802487023174763\n",
      "Loss AMM, iter 102: 0.006043999921530485\n",
      "Loss AMM, iter 103: 0.00595894455909729\n",
      "Loss AMM, iter 104: 0.005560222547501326\n",
      "Loss AMM, iter 105: 0.005239720921963453\n",
      "Loss AMM, iter 106: 0.00496029993519187\n",
      "Loss AMM, iter 107: 0.004853024613112211\n",
      "Loss AMM, iter 108: 0.00493817450478673\n",
      "Loss AMM, iter 109: 0.005219738464802504\n",
      "Loss AMM, iter 110: 0.004380678292363882\n",
      "Loss AMM, iter 111: 0.004306466318666935\n",
      "Loss AMM, iter 112: 0.004364662803709507\n",
      "Loss AMM, iter 113: 0.004578523803502321\n",
      "Loss AMM, iter 114: 0.004151542205363512\n",
      "Loss AMM, iter 115: 0.0041702911257743835\n",
      "Loss AMM, iter 116: 0.0037538674660027027\n",
      "Loss AMM, iter 117: 0.003702618880197406\n",
      "Loss AMM, iter 118: 0.003867592429742217\n",
      "Loss AMM, iter 119: 0.003635865170508623\n",
      "Loss AMM, iter 120: 0.0034257390070706606\n",
      "Loss AMM, iter 121: 0.003937601577490568\n",
      "Loss AMM, iter 122: 0.0033689122647047043\n",
      "Loss AMM, iter 123: 0.003266391111537814\n",
      "Loss AMM, iter 124: 0.0031972515862435102\n",
      "Loss AMM, iter 125: 0.003426911076530814\n",
      "Loss AMM, iter 126: 0.0032725303899496794\n",
      "Loss AMM, iter 127: 0.0033815987408161163\n",
      "Loss AMM, iter 128: 0.0028520184569060802\n",
      "Loss AMM, iter 129: 0.0030038594268262386\n",
      "Loss AMM, iter 130: 0.003267002059146762\n",
      "Loss AMM, iter 131: 0.0027767138089984655\n",
      "Loss AMM, iter 132: 0.002882940461859107\n",
      "Loss AMM, iter 133: 0.00354757159948349\n",
      "Loss AMM, iter 134: 0.002760967006906867\n",
      "Loss AMM, iter 135: 0.0025785635225474834\n",
      "Loss AMM, iter 136: 0.0026750548277050257\n",
      "Loss AMM, iter 137: 0.002929673995822668\n",
      "Loss AMM, iter 138: 0.0024734199978411198\n",
      "Loss AMM, iter 139: 0.0028026073705404997\n",
      "Loss AMM, iter 140: 0.00275208055973053\n",
      "Loss AMM, iter 141: 0.0022665683645755053\n",
      "Loss AMM, iter 142: 0.0021974944975227118\n",
      "Loss AMM, iter 143: 0.00238751876167953\n",
      "Loss AMM, iter 144: 0.0025549475103616714\n",
      "Loss AMM, iter 145: 0.0029207514598965645\n",
      "Loss AMM, iter 146: 0.002244550036266446\n",
      "Loss AMM, iter 147: 0.0021602062042802572\n",
      "Loss AMM, iter 148: 0.0021930683869868517\n",
      "Loss AMM, iter 149: 0.002608601003885269\n",
      "Loss AMM, iter 150: 0.002062687184661627\n",
      "Loss AMM, iter 151: 0.002286433707922697\n",
      "Loss AMM, iter 152: 0.0020651700906455517\n",
      "Loss AMM, iter 153: 0.0023055137135088444\n",
      "Loss AMM, iter 154: 0.0019723556470125914\n",
      "Loss AMM, iter 155: 0.0020980744156986475\n",
      "Loss AMM, iter 156: 0.002084714127704501\n",
      "Loss AMM, iter 157: 0.0021902150474488735\n",
      "Loss AMM, iter 158: 0.0017418559873476624\n",
      "Loss AMM, iter 159: 0.0021143786143511534\n",
      "Loss AMM, iter 160: 0.0017691837856546044\n",
      "Loss AMM, iter 161: 0.0021701480727642775\n",
      "Loss AMM, iter 162: 0.001881705247797072\n",
      "Loss AMM, iter 163: 0.0019719123374670744\n",
      "Loss AMM, iter 164: 0.0016697567189112306\n",
      "Loss AMM, iter 165: 0.001638004556298256\n",
      "Loss AMM, iter 166: 0.0017861071974039078\n",
      "Loss AMM, iter 167: 0.001530793379060924\n",
      "Loss AMM, iter 168: 0.001600479707121849\n",
      "Loss AMM, iter 169: 0.0017631987575441599\n",
      "Loss AMM, iter 170: 0.0020357288885861635\n",
      "Loss AMM, iter 171: 0.0016657738015055656\n",
      "Loss AMM, iter 172: 0.0015449518105015159\n",
      "Loss AMM, iter 173: 0.0016761840088292956\n",
      "Loss AMM, iter 174: 0.0013952440349385142\n",
      "Loss AMM, iter 175: 0.0014599038986489177\n",
      "Loss AMM, iter 176: 0.0016054882435128093\n",
      "Loss AMM, iter 177: 0.0014271605759859085\n",
      "Loss AMM, iter 178: 0.0013126566773280501\n",
      "Loss AMM, iter 179: 0.0015255610924214125\n",
      "Loss AMM, iter 180: 0.0016383121255785227\n",
      "Loss AMM, iter 181: 0.0018066506600007415\n",
      "Loss AMM, iter 182: 0.0015704516554251313\n",
      "Loss AMM, iter 183: 0.0014116083038970828\n",
      "Loss AMM, iter 184: 0.0013450031401589513\n",
      "Loss AMM, iter 185: 0.0014232045505195856\n",
      "Loss AMM, iter 186: 0.0013854537392035127\n",
      "Loss AMM, iter 187: 0.0011423489777371287\n",
      "Loss AMM, iter 188: 0.001591994659975171\n",
      "Loss AMM, iter 189: 0.001240222598426044\n",
      "Loss AMM, iter 190: 0.0014483754057437181\n",
      "Loss AMM, iter 191: 0.0017117485404014587\n",
      "Loss AMM, iter 192: 0.00131218321621418\n",
      "Loss AMM, iter 193: 0.0012833911459892988\n",
      "Loss AMM, iter 194: 0.0014852358726784587\n",
      "Loss AMM, iter 195: 0.0011206442723050714\n",
      "Loss AMM, iter 196: 0.0010520826326683164\n",
      "Loss AMM, iter 197: 0.0014239844167605042\n",
      "Loss AMM, iter 198: 0.0011883348925039172\n",
      "Loss AMM, iter 199: 0.0011099546682089567\n",
      "Loss AMM, iter 200: 0.0011313229333609343\n",
      "Loss AMM, iter 201: 0.0013170597376301885\n",
      "Loss AMM, iter 202: 0.001091981423087418\n",
      "Loss AMM, iter 203: 0.001132940175011754\n",
      "Loss AMM, iter 204: 0.0010153761832043529\n",
      "Loss AMM, iter 205: 0.0010444785002619028\n",
      "Loss AMM, iter 206: 0.001158343511633575\n",
      "Loss AMM, iter 207: 0.0010284200543537736\n",
      "Loss AMM, iter 208: 0.0010888727847486734\n",
      "Loss AMM, iter 209: 0.0011258403537794948\n",
      "Loss AMM, iter 210: 0.001086418516933918\n",
      "Loss AMM, iter 211: 0.0011880266247317195\n",
      "Loss AMM, iter 212: 0.0011075307847931981\n",
      "Loss AMM, iter 213: 0.0010590433375909925\n",
      "Loss AMM, iter 214: 0.0009286277927458286\n",
      "Loss AMM, iter 215: 0.0011644762707874179\n",
      "Loss AMM, iter 216: 0.0009613125002942979\n",
      "Loss AMM, iter 217: 0.0011203206377103925\n",
      "Loss AMM, iter 218: 0.0011638256255537271\n",
      "Loss AMM, iter 219: 0.0009305172716267407\n",
      "Loss AMM, iter 220: 0.001296234899200499\n",
      "Loss AMM, iter 221: 0.0011796915205195546\n",
      "Loss AMM, iter 222: 0.0010485228849574924\n",
      "Loss AMM, iter 223: 0.0008446513093076646\n",
      "Loss AMM, iter 224: 0.0011679594172164798\n",
      "Loss AMM, iter 225: 0.0009619275224395096\n",
      "Loss AMM, iter 226: 0.0009186050156131387\n",
      "Loss AMM, iter 227: 0.0009561434271745384\n",
      "Loss AMM, iter 228: 0.001038090675137937\n",
      "Loss AMM, iter 229: 0.0011015551863238215\n",
      "Loss AMM, iter 230: 0.0009128924575634301\n",
      "Loss AMM, iter 231: 0.0009613811271265149\n",
      "Loss AMM, iter 232: 0.0009754486382007599\n",
      "Loss AMM, iter 233: 0.0008159272256307304\n",
      "Loss AMM, iter 234: 0.0008804451790638268\n",
      "Loss AMM, iter 235: 0.0009648526320233941\n",
      "Loss AMM, iter 236: 0.0008483888814225793\n",
      "Loss AMM, iter 237: 0.0009934342233464122\n",
      "Loss AMM, iter 238: 0.000777364824898541\n",
      "Loss AMM, iter 239: 0.0007365667843259871\n",
      "Loss AMM, iter 240: 0.0008446999127045274\n",
      "Loss AMM, iter 241: 0.0008301217458210886\n",
      "Loss AMM, iter 242: 0.000951258116401732\n",
      "Loss AMM, iter 243: 0.000768714293371886\n",
      "Loss AMM, iter 244: 0.0008435279014520347\n",
      "Loss AMM, iter 245: 0.0008661161409690976\n",
      "Loss AMM, iter 246: 0.0007999419467523694\n",
      "Loss AMM, iter 247: 0.0007367227808572352\n",
      "Loss AMM, iter 248: 0.0007782158791087568\n",
      "Loss AMM, iter 249: 0.0007955242181196809\n",
      "Loss AMM, iter 250: 0.0008133975206874311\n",
      "Loss AMM, iter 251: 0.0007784338085912168\n",
      "Loss AMM, iter 252: 0.0007973811007104814\n",
      "Loss AMM, iter 253: 0.0007356991991400719\n",
      "Loss AMM, iter 254: 0.0006997715099714696\n",
      "Loss AMM, iter 255: 0.0008776819449849427\n",
      "Loss AMM, iter 256: 0.0006754885544069111\n",
      "Loss AMM, iter 257: 0.0007318339194171131\n",
      "Loss AMM, iter 258: 0.0006725858547724783\n",
      "Loss AMM, iter 259: 0.0006871312507428229\n",
      "Loss AMM, iter 260: 0.0007365375640802085\n",
      "Loss AMM, iter 261: 0.0007451814017258584\n",
      "Loss AMM, iter 262: 0.0006363838911056519\n",
      "Loss AMM, iter 263: 0.0007009354885667562\n",
      "Loss AMM, iter 264: 0.0006899088039062917\n",
      "Loss AMM, iter 265: 0.0008399593643844128\n",
      "Loss AMM, iter 266: 0.000709917105268687\n",
      "Loss AMM, iter 267: 0.0007031533168628812\n",
      "Loss AMM, iter 268: 0.0007353461114689708\n",
      "Loss AMM, iter 269: 0.0005489707109518349\n",
      "Loss AMM, iter 270: 0.0006777238450013101\n",
      "Loss AMM, iter 271: 0.0005797026096843183\n",
      "Loss AMM, iter 272: 0.0006773429340682924\n",
      "Loss AMM, iter 273: 0.0006609187112189829\n",
      "Loss AMM, iter 274: 0.0006801497074775398\n",
      "Loss AMM, iter 275: 0.0005736598977819085\n",
      "Loss AMM, iter 276: 0.0005825653206557035\n",
      "Loss AMM, iter 277: 0.0006636581965722144\n",
      "Loss AMM, iter 278: 0.0007426110678352416\n",
      "Loss AMM, iter 279: 0.0007393858977593482\n",
      "Loss AMM, iter 280: 0.0005603251047432423\n",
      "Loss AMM, iter 281: 0.000565891619771719\n",
      "Loss AMM, iter 282: 0.0008436273201368749\n",
      "Loss AMM, iter 283: 0.0006059338338673115\n",
      "Loss AMM, iter 284: 0.0005863988772034645\n",
      "Loss AMM, iter 285: 0.0006078257574699819\n",
      "Loss AMM, iter 286: 0.00065393972909078\n",
      "Loss AMM, iter 287: 0.0006550877587869763\n",
      "Loss AMM, iter 288: 0.0006559800240211189\n",
      "Loss AMM, iter 289: 0.0005743587971664965\n",
      "Loss AMM, iter 290: 0.0005529592162929475\n",
      "Loss AMM, iter 291: 0.0005170287913642824\n",
      "Loss AMM, iter 292: 0.0005523678846657276\n",
      "Loss AMM, iter 293: 0.0005534027586691082\n",
      "Loss AMM, iter 294: 0.0006759326788596809\n",
      "Loss AMM, iter 295: 0.0005344252567738295\n",
      "Loss AMM, iter 296: 0.0007272073999047279\n",
      "Loss AMM, iter 297: 0.0005937787354923785\n",
      "Loss AMM, iter 298: 0.0006231571896933019\n",
      "Loss AMM, iter 299: 0.0006114226416684687\n",
      "Loss AMM, iter 300: 0.0005464911228045821\n",
      "Loss AMM, iter 301: 0.000593601493164897\n",
      "Loss AMM, iter 302: 0.0005769155686721206\n",
      "Loss AMM, iter 303: 0.000503388000652194\n",
      "Loss AMM, iter 304: 0.0004768165817949921\n",
      "Loss AMM, iter 305: 0.000475738343084231\n",
      "Loss AMM, iter 306: 0.0005455765640363097\n",
      "Loss AMM, iter 307: 0.000557240447960794\n",
      "Loss AMM, iter 308: 0.0005260606412775815\n",
      "Loss AMM, iter 309: 0.0004598148225340992\n",
      "Loss AMM, iter 310: 0.0006030722870491445\n",
      "Loss AMM, iter 311: 0.000548661919310689\n",
      "Loss AMM, iter 312: 0.0006208924460224807\n",
      "Loss AMM, iter 313: 0.000532213132828474\n",
      "Loss AMM, iter 314: 0.00048526018508709967\n",
      "Loss AMM, iter 315: 0.00043867045314982533\n",
      "Loss AMM, iter 316: 0.0004512546584010124\n",
      "Loss AMM, iter 317: 0.0005321333301253617\n",
      "Loss AMM, iter 318: 0.0005271504051052034\n",
      "Loss AMM, iter 319: 0.0004507770936470479\n",
      "Loss AMM, iter 320: 0.0005761481006629765\n",
      "Loss AMM, iter 321: 0.00045688505633734167\n",
      "Loss AMM, iter 322: 0.0005393251194618642\n",
      "Loss AMM, iter 323: 0.0004308353818487376\n",
      "Loss AMM, iter 324: 0.0005015993374399841\n",
      "Loss AMM, iter 325: 0.000563506968319416\n",
      "Loss AMM, iter 326: 0.0004533178289420903\n",
      "Loss AMM, iter 327: 0.0005154315731488168\n",
      "Loss AMM, iter 328: 0.0005404590046964586\n",
      "Loss AMM, iter 329: 0.0004757348506245762\n",
      "Loss AMM, iter 330: 0.00043784253648482263\n",
      "Loss AMM, iter 331: 0.000448835373390466\n",
      "Loss AMM, iter 332: 0.0003819435660261661\n",
      "Loss AMM, iter 333: 0.0005440096720121801\n",
      "Loss AMM, iter 334: 0.00045667478116229177\n",
      "Loss AMM, iter 335: 0.0004720382858067751\n",
      "Loss AMM, iter 336: 0.0004889412666670978\n",
      "Loss AMM, iter 337: 0.00045182553003542125\n",
      "Loss AMM, iter 338: 0.0004603432898875326\n",
      "Loss AMM, iter 339: 0.0005194764817133546\n",
      "Loss AMM, iter 340: 0.00041341898031532764\n",
      "Loss AMM, iter 341: 0.0003954808635171503\n",
      "Loss AMM, iter 342: 0.0003954096173401922\n",
      "Loss AMM, iter 343: 0.0003757553349714726\n",
      "Loss AMM, iter 344: 0.000400651158997789\n",
      "Loss AMM, iter 345: 0.0004492510051932186\n",
      "Loss AMM, iter 346: 0.00048447700100950897\n",
      "Loss AMM, iter 347: 0.0005230244132690132\n",
      "Loss AMM, iter 348: 0.000456919486168772\n",
      "Loss AMM, iter 349: 0.0004095474723726511\n",
      "Loss AMM, iter 350: 0.0004847834352403879\n",
      "Loss AMM, iter 351: 0.0004053594602737576\n",
      "Loss AMM, iter 352: 0.0005051774787716568\n",
      "Loss AMM, iter 353: 0.0003870454092975706\n",
      "Loss AMM, iter 354: 0.0004175838257651776\n",
      "Loss AMM, iter 355: 0.0003465894260443747\n",
      "Loss AMM, iter 356: 0.00045192940160632133\n",
      "Loss AMM, iter 357: 0.0004924705135636032\n",
      "Loss AMM, iter 358: 0.0004779925220645964\n",
      "Loss AMM, iter 359: 0.00035489050787873566\n",
      "Loss AMM, iter 360: 0.00041153604979626834\n",
      "Loss AMM, iter 361: 0.0004551091406028718\n",
      "Loss AMM, iter 362: 0.0004075815377291292\n",
      "Loss AMM, iter 363: 0.00040405578329227865\n",
      "Loss AMM, iter 364: 0.00040969785186462104\n",
      "Loss AMM, iter 365: 0.0003883959143422544\n",
      "Loss AMM, iter 366: 0.0004133093752898276\n",
      "Loss AMM, iter 367: 0.0003977651649620384\n",
      "Loss AMM, iter 368: 0.0003967710945289582\n",
      "Loss AMM, iter 369: 0.00042556223343126476\n",
      "Loss AMM, iter 370: 0.0004188059829175472\n",
      "Loss AMM, iter 371: 0.0003811198694165796\n",
      "Loss AMM, iter 372: 0.00036811348400078714\n",
      "Loss AMM, iter 373: 0.00037878743023611605\n",
      "Loss AMM, iter 374: 0.0004722012672573328\n",
      "Loss AMM, iter 375: 0.0004501051444094628\n",
      "Loss AMM, iter 376: 0.00038219819543883204\n",
      "Loss AMM, iter 377: 0.00038881032378412783\n",
      "Loss AMM, iter 378: 0.00036710494896396995\n",
      "Loss AMM, iter 379: 0.00041758763836696744\n",
      "Loss AMM, iter 380: 0.0003645599354058504\n",
      "Loss AMM, iter 381: 0.00040608830749988556\n",
      "Loss AMM, iter 382: 0.0003114218998234719\n",
      "Loss AMM, iter 383: 0.00047498554340563715\n",
      "Loss AMM, iter 384: 0.00043825802276842296\n",
      "Loss AMM, iter 385: 0.00036948369233869016\n",
      "Loss AMM, iter 386: 0.0003618363698478788\n",
      "Loss AMM, iter 387: 0.0004224523436278105\n",
      "Loss AMM, iter 388: 0.00035282503813505173\n",
      "Loss AMM, iter 389: 0.00039067622856236994\n",
      "Loss AMM, iter 390: 0.00045591415255330503\n",
      "Loss AMM, iter 391: 0.0003738369850907475\n",
      "Loss AMM, iter 392: 0.0003146866802126169\n",
      "Loss AMM, iter 393: 0.0003362879215274006\n",
      "Loss AMM, iter 394: 0.00038413863512687385\n",
      "Loss AMM, iter 395: 0.0003120528708677739\n",
      "Loss AMM, iter 396: 0.00033276923932135105\n",
      "Loss AMM, iter 397: 0.00035913288593292236\n",
      "Loss AMM, iter 398: 0.0003436760453041643\n",
      "Loss AMM, iter 399: 0.0003253189497627318\n",
      "Loss AMM, iter 400: 0.0003004998143296689\n",
      "Loss AMM, iter 401: 0.00035517552169039845\n",
      "Loss AMM, iter 402: 0.0004100164514966309\n",
      "Loss AMM, iter 403: 0.0003284020349383354\n",
      "Loss AMM, iter 404: 0.00035705819027498364\n",
      "Loss AMM, iter 405: 0.00031775570823810995\n",
      "Loss AMM, iter 406: 0.00038153058267198503\n",
      "Loss AMM, iter 407: 0.0003661087539512664\n",
      "Loss AMM, iter 408: 0.00033913584775291383\n",
      "Loss AMM, iter 409: 0.0003280620730947703\n",
      "Loss AMM, iter 410: 0.0003841583093162626\n",
      "Loss AMM, iter 411: 0.0002952191571239382\n",
      "Loss AMM, iter 412: 0.00031660794047638774\n",
      "Loss AMM, iter 413: 0.0004012415593024343\n",
      "Loss AMM, iter 414: 0.0002936969685833901\n",
      "Loss AMM, iter 415: 0.0003244491235818714\n",
      "Loss AMM, iter 416: 0.00038106116699054837\n",
      "Loss AMM, iter 417: 0.0003727725415956229\n",
      "Loss AMM, iter 418: 0.00032318616285920143\n",
      "Loss AMM, iter 419: 0.0003309449821244925\n",
      "Loss AMM, iter 420: 0.0002714904258027673\n",
      "Loss AMM, iter 421: 0.0003318871895316988\n",
      "Loss AMM, iter 422: 0.00028766586910933256\n",
      "Loss AMM, iter 423: 0.0003249956062063575\n",
      "Loss AMM, iter 424: 0.00041599106043577194\n",
      "Loss AMM, iter 425: 0.00032042842940427363\n",
      "Loss AMM, iter 426: 0.0003303457924630493\n",
      "Loss AMM, iter 427: 0.0003100945905316621\n",
      "Loss AMM, iter 428: 0.0002995826362166554\n",
      "Loss AMM, iter 429: 0.0003027569327969104\n",
      "Loss AMM, iter 430: 0.0002916128723882139\n",
      "Loss AMM, iter 431: 0.0002795265754684806\n",
      "Loss AMM, iter 432: 0.000307656911900267\n",
      "Loss AMM, iter 433: 0.0002867970324587077\n",
      "Loss AMM, iter 434: 0.00025738883414305747\n",
      "Loss AMM, iter 435: 0.0003035711997654289\n",
      "Loss AMM, iter 436: 0.000358319201041013\n",
      "Loss AMM, iter 437: 0.0002994760579895228\n",
      "Loss AMM, iter 438: 0.0003032306849490851\n",
      "Loss AMM, iter 439: 0.00031437716097570956\n",
      "Loss AMM, iter 440: 0.00031878642039373517\n",
      "Loss AMM, iter 441: 0.00027917633997276425\n",
      "Loss AMM, iter 442: 0.0002717495372053236\n",
      "Loss AMM, iter 443: 0.00031066412338986993\n",
      "Loss AMM, iter 444: 0.0002765118551906198\n",
      "Loss AMM, iter 445: 0.00033965217880904675\n",
      "Loss AMM, iter 446: 0.00030576359131373465\n",
      "Loss AMM, iter 447: 0.00025851171812973917\n",
      "Loss AMM, iter 448: 0.00025554621242918074\n",
      "Loss AMM, iter 449: 0.00030995378619991243\n",
      "Loss AMM, iter 450: 0.00033306374098174274\n",
      "Loss AMM, iter 451: 0.0003212574520148337\n",
      "Loss AMM, iter 452: 0.00023854454047977924\n",
      "Loss AMM, iter 453: 0.00029277231078594923\n",
      "Loss AMM, iter 454: 0.0003011605003848672\n",
      "Loss AMM, iter 455: 0.00035273804678581655\n",
      "Loss AMM, iter 456: 0.00029809557599946856\n",
      "Loss AMM, iter 457: 0.00027332809986546636\n",
      "Loss AMM, iter 458: 0.0002973285154439509\n",
      "Loss AMM, iter 459: 0.0002567213377915323\n",
      "Loss AMM, iter 460: 0.00028217255021445453\n",
      "Loss AMM, iter 461: 0.0003054908011108637\n",
      "Loss AMM, iter 462: 0.00036485795862972736\n",
      "Loss AMM, iter 463: 0.00022871104010846466\n",
      "Loss AMM, iter 464: 0.0002555666142143309\n",
      "Loss AMM, iter 465: 0.00034151397994719446\n",
      "Loss AMM, iter 466: 0.0002813671017065644\n",
      "Loss AMM, iter 467: 0.0002673372218851\n",
      "Loss AMM, iter 468: 0.0002672797127161175\n",
      "Loss AMM, iter 469: 0.00034880914608947933\n",
      "Loss AMM, iter 470: 0.00026866348343901336\n",
      "Loss AMM, iter 471: 0.00031026272336021066\n",
      "Loss AMM, iter 472: 0.0002785077958833426\n",
      "Loss AMM, iter 473: 0.0002565918257459998\n",
      "Loss AMM, iter 474: 0.0002531114441808313\n",
      "Loss AMM, iter 475: 0.00024587588268332183\n",
      "Loss AMM, iter 476: 0.0002687646774575114\n",
      "Loss AMM, iter 477: 0.000232802820391953\n",
      "Loss AMM, iter 478: 0.0002882627013605088\n",
      "Loss AMM, iter 479: 0.0003054163826163858\n",
      "Loss AMM, iter 480: 0.0002669843379408121\n",
      "Loss AMM, iter 481: 0.00026599629200063646\n",
      "Loss AMM, iter 482: 0.0003136000595986843\n",
      "Loss AMM, iter 483: 0.00023544311989098787\n",
      "Loss AMM, iter 484: 0.00023912196047604084\n",
      "Loss AMM, iter 485: 0.0003058600705116987\n",
      "Loss AMM, iter 486: 0.00020752901036757976\n",
      "Loss AMM, iter 487: 0.00024453061632812023\n",
      "Loss AMM, iter 488: 0.0002040228428086266\n",
      "Loss AMM, iter 489: 0.000248504540650174\n",
      "Loss AMM, iter 490: 0.00025627980357967317\n",
      "Loss AMM, iter 491: 0.00023245257034432143\n",
      "Loss AMM, iter 492: 0.00020671366655733436\n",
      "Loss AMM, iter 493: 0.00021940705482847989\n",
      "Loss AMM, iter 494: 0.00022790029470343143\n",
      "Loss AMM, iter 495: 0.00023375815362669528\n",
      "Loss AMM, iter 496: 0.0002004119596676901\n",
      "Loss AMM, iter 497: 0.00028714368818327785\n",
      "Loss AMM, iter 498: 0.00027241482166573405\n",
      "Loss AMM, iter 499: 0.00020910096645820886\n",
      "Loss AMM, iter 500: 0.00025117589393630624\n",
      "Loss AMM, iter 501: 0.00028362273587845266\n",
      "Loss AMM, iter 502: 0.00020957888045813888\n",
      "Loss AMM, iter 503: 0.00025192167959176004\n",
      "Loss AMM, iter 504: 0.0002209007361670956\n",
      "Loss AMM, iter 505: 0.0003646985860541463\n",
      "Loss AMM, iter 506: 0.00021414468938019127\n",
      "Loss AMM, iter 507: 0.0002856861974578351\n",
      "Loss AMM, iter 508: 0.0002320252388017252\n",
      "Loss AMM, iter 509: 0.00023244565818458796\n",
      "Loss AMM, iter 510: 0.00021311140153557062\n",
      "Loss AMM, iter 511: 0.00021943355386611074\n",
      "Loss AMM, iter 512: 0.00021996701252646744\n",
      "Loss AMM, iter 513: 0.00022750304196961224\n",
      "Loss AMM, iter 514: 0.00023075738863553852\n",
      "Loss AMM, iter 515: 0.00018513124086894095\n",
      "Loss AMM, iter 516: 0.00023644327302463353\n",
      "Loss AMM, iter 517: 0.00023039970255922526\n",
      "Loss AMM, iter 518: 0.0002272175916004926\n",
      "Loss AMM, iter 519: 0.00020910378952976316\n",
      "Loss AMM, iter 520: 0.0002018969098571688\n",
      "Loss AMM, iter 521: 0.000255676539381966\n",
      "Loss AMM, iter 522: 0.00023829424753785133\n",
      "Loss AMM, iter 523: 0.00022279431868810207\n",
      "Loss AMM, iter 524: 0.00022047109086997807\n",
      "Loss AMM, iter 525: 0.00019871511904057115\n",
      "Loss AMM, iter 526: 0.00025692282360978425\n",
      "Loss AMM, iter 527: 0.0002307862596353516\n",
      "Loss AMM, iter 528: 0.0002795912732835859\n",
      "Loss AMM, iter 529: 0.00019064967636950314\n",
      "Loss AMM, iter 530: 0.00017269157979171723\n",
      "Loss AMM, iter 531: 0.00023868877906352282\n",
      "Loss AMM, iter 532: 0.00018421292770653963\n",
      "Loss AMM, iter 533: 0.00024618147290311754\n",
      "Loss AMM, iter 534: 0.00017010867304634303\n",
      "Loss AMM, iter 535: 0.00017064475105144083\n",
      "Loss AMM, iter 536: 0.0002672645205166191\n",
      "Loss AMM, iter 537: 0.0002441364631522447\n",
      "Loss AMM, iter 538: 0.00018398830434307456\n",
      "Loss AMM, iter 539: 0.00022002500190865248\n",
      "Loss AMM, iter 540: 0.00018690802971832454\n",
      "Loss AMM, iter 541: 0.00022716929379384965\n",
      "Loss AMM, iter 542: 0.00019539821369107813\n",
      "Loss AMM, iter 543: 0.00021676206961274147\n",
      "Loss AMM, iter 544: 0.00023649181821383536\n",
      "Loss AMM, iter 545: 0.00016986482660286129\n",
      "Loss AMM, iter 546: 0.0002064254367724061\n",
      "Loss AMM, iter 547: 0.0001971791934920475\n",
      "Loss AMM, iter 548: 0.00019813724793493748\n",
      "Loss AMM, iter 549: 0.00018959620501846075\n",
      "Loss AMM, iter 550: 0.00017721722542773932\n",
      "Loss AMM, iter 551: 0.00019882770720869303\n",
      "Loss AMM, iter 552: 0.00019657374650705606\n",
      "Loss AMM, iter 553: 0.00021384670981206\n",
      "Loss AMM, iter 554: 0.00016869335377123207\n",
      "Loss AMM, iter 555: 0.00020434451289474964\n",
      "Loss AMM, iter 556: 0.00018170507973991334\n",
      "Loss AMM, iter 557: 0.00021916782134212554\n",
      "Loss AMM, iter 558: 0.0002731194254010916\n",
      "Loss AMM, iter 559: 0.000193344647414051\n",
      "Loss AMM, iter 560: 0.0002060313563561067\n",
      "Loss AMM, iter 561: 0.0001793548435671255\n",
      "Loss AMM, iter 562: 0.00017288913659285754\n",
      "Loss AMM, iter 563: 0.00019168922153767198\n",
      "Loss AMM, iter 564: 0.00018404866568744183\n",
      "Loss AMM, iter 565: 0.00020498571393545717\n",
      "Loss AMM, iter 566: 0.0001763247710186988\n",
      "Loss AMM, iter 567: 0.00017383448721375316\n",
      "Loss AMM, iter 568: 0.000178878937731497\n",
      "Loss AMM, iter 569: 0.0001897873735288158\n",
      "Loss AMM, iter 570: 0.00018886648467741907\n",
      "Loss AMM, iter 571: 0.00017683424812275916\n",
      "Loss AMM, iter 572: 0.00024068483617156744\n",
      "Loss AMM, iter 573: 0.000243205446167849\n",
      "Loss AMM, iter 574: 0.000212269471376203\n",
      "Loss AMM, iter 575: 0.0001898885384434834\n",
      "Loss AMM, iter 576: 0.00022228453599382192\n",
      "Loss AMM, iter 577: 0.00018901799921877682\n",
      "Loss AMM, iter 578: 0.0001657953835092485\n",
      "Loss AMM, iter 579: 0.00018689669377636164\n",
      "Loss AMM, iter 580: 0.00018713458848651499\n",
      "Loss AMM, iter 581: 0.000224345363676548\n",
      "Loss AMM, iter 582: 0.0002157776616513729\n",
      "Loss AMM, iter 583: 0.00017527365707792342\n",
      "Loss AMM, iter 584: 0.0001548130385344848\n",
      "Loss AMM, iter 585: 0.000187878220458515\n",
      "Loss AMM, iter 586: 0.0002205085038440302\n",
      "Loss AMM, iter 587: 0.0001593508495716378\n",
      "Loss AMM, iter 588: 0.0001823515340220183\n",
      "Loss AMM, iter 589: 0.00016405244241468608\n",
      "Loss AMM, iter 590: 0.0001720143191050738\n",
      "Loss AMM, iter 591: 0.000211324542760849\n",
      "Loss AMM, iter 592: 0.0002017213118961081\n",
      "Loss AMM, iter 593: 0.00015838518447708338\n",
      "Loss AMM, iter 594: 0.0002201729075750336\n",
      "Loss AMM, iter 595: 0.00018305420235265046\n",
      "Loss AMM, iter 596: 0.00017237923748325557\n",
      "Loss AMM, iter 597: 0.00017333193682134151\n",
      "Loss AMM, iter 598: 0.00021224052761681378\n",
      "Loss AMM, iter 599: 0.00018125415954273194\n",
      "Loss AMM, iter 600: 0.00018375243234913796\n",
      "Loss AMM, iter 601: 0.00017550190386828035\n",
      "Loss AMM, iter 602: 0.00019663436978589743\n",
      "Loss AMM, iter 603: 0.00017227769421879202\n",
      "Loss AMM, iter 604: 0.00015833468933124095\n",
      "Loss AMM, iter 605: 0.0001953181199496612\n",
      "Loss AMM, iter 606: 0.00017032958567142487\n",
      "Loss AMM, iter 607: 0.0001453574950573966\n",
      "Loss AMM, iter 608: 0.0001718270796118304\n",
      "Loss AMM, iter 609: 0.00016618482186459005\n",
      "Loss AMM, iter 610: 0.00016642075206618756\n",
      "Loss AMM, iter 611: 0.00013818532170262188\n",
      "Loss AMM, iter 612: 0.00018025122699327767\n",
      "Loss AMM, iter 613: 0.0001675995154073462\n",
      "Loss AMM, iter 614: 0.00020066396973561496\n",
      "Loss AMM, iter 615: 0.0001905370590975508\n",
      "Loss AMM, iter 616: 0.00016315955144818872\n",
      "Loss AMM, iter 617: 0.00017734123684931546\n",
      "Loss AMM, iter 618: 0.00015975117275957018\n",
      "Loss AMM, iter 619: 0.00017763183859642595\n",
      "Loss AMM, iter 620: 0.00017966206360142678\n",
      "Loss AMM, iter 621: 0.0001839480100898072\n",
      "Loss AMM, iter 622: 0.00018069904763251543\n",
      "Loss AMM, iter 623: 0.0001919008354889229\n",
      "Loss AMM, iter 624: 0.00014908891171216965\n",
      "Loss AMM, iter 625: 0.00018112426914740354\n",
      "Loss AMM, iter 626: 0.00016522091755177826\n",
      "Loss AMM, iter 627: 0.0001498897181591019\n",
      "Loss AMM, iter 628: 0.00022088868718128651\n",
      "Loss AMM, iter 629: 0.00020365466480143368\n",
      "Loss AMM, iter 630: 0.00019350646471139044\n",
      "Loss AMM, iter 631: 0.00017323937208857387\n",
      "Loss AMM, iter 632: 0.00021580896282102913\n",
      "Loss AMM, iter 633: 0.00015590374823659658\n",
      "Loss AMM, iter 634: 0.0001692908554105088\n",
      "Loss AMM, iter 635: 0.0002118528209393844\n",
      "Loss AMM, iter 636: 0.00018765444110613316\n",
      "Loss AMM, iter 637: 0.00022080946655478328\n",
      "Loss AMM, iter 638: 0.00018514030671212822\n",
      "Loss AMM, iter 639: 0.00017357237811665982\n",
      "Loss AMM, iter 640: 0.00014805406681261957\n",
      "Loss AMM, iter 641: 0.00013391167158260942\n",
      "Loss AMM, iter 642: 0.00017365138046443462\n",
      "Loss AMM, iter 643: 0.00014888575242366642\n",
      "Loss AMM, iter 644: 0.0001381862093694508\n",
      "Loss AMM, iter 645: 0.00011999602429568768\n",
      "Loss AMM, iter 646: 0.00020805225358344615\n",
      "Loss AMM, iter 647: 0.00014875495980959386\n",
      "Loss AMM, iter 648: 0.00012450477515812963\n",
      "Loss AMM, iter 649: 0.00014790262503083795\n",
      "Loss AMM, iter 650: 0.00014876843488309532\n",
      "Loss AMM, iter 651: 0.00012730085290968418\n",
      "Loss AMM, iter 652: 0.00017045484855771065\n",
      "Loss AMM, iter 653: 0.0001310313818976283\n",
      "Loss AMM, iter 654: 0.00014344239025376737\n",
      "Loss AMM, iter 655: 0.00015781847469042987\n",
      "Loss AMM, iter 656: 0.00019263102149125189\n",
      "Loss AMM, iter 657: 0.0001336834829999134\n",
      "Loss AMM, iter 658: 0.00014607618504669517\n",
      "Loss AMM, iter 659: 0.000154148816363886\n",
      "Loss AMM, iter 660: 0.00017033527547027916\n",
      "Loss AMM, iter 661: 0.00014063027629163116\n",
      "Loss AMM, iter 662: 0.00016305588360410184\n",
      "Loss AMM, iter 663: 0.00015118673036340624\n",
      "Loss AMM, iter 664: 0.00013132802268955857\n",
      "Loss AMM, iter 665: 0.00013455809676088393\n",
      "Loss AMM, iter 666: 0.00016904939548112452\n",
      "Loss AMM, iter 667: 0.0001346767385257408\n",
      "Loss AMM, iter 668: 0.00018141917826142162\n",
      "Loss AMM, iter 669: 0.00015825506125111133\n",
      "Loss AMM, iter 670: 0.00013215640501584858\n",
      "Loss AMM, iter 671: 0.00019031160627491772\n",
      "Loss AMM, iter 672: 0.0001794882264221087\n",
      "Loss AMM, iter 673: 0.00013427092926576734\n",
      "Loss AMM, iter 674: 0.0001432206336176023\n",
      "Loss AMM, iter 675: 0.00013486042735166848\n",
      "Loss AMM, iter 676: 0.00020738612511195242\n",
      "Loss AMM, iter 677: 0.00012156892626080662\n",
      "Loss AMM, iter 678: 0.00021473584638442844\n",
      "Loss AMM, iter 679: 0.00014473671035375446\n",
      "Loss AMM, iter 680: 0.00015305187844205648\n",
      "Loss AMM, iter 681: 0.0001562579709570855\n",
      "Loss AMM, iter 682: 0.00017068996385205537\n",
      "Loss AMM, iter 683: 0.00013846879301127046\n",
      "Loss AMM, iter 684: 0.0001343723270110786\n",
      "Loss AMM, iter 685: 0.00013744733587373048\n",
      "Loss AMM, iter 686: 0.00011820306826848537\n",
      "Loss AMM, iter 687: 0.00015091946988832206\n",
      "Loss AMM, iter 688: 0.00012178542237961665\n",
      "Loss AMM, iter 689: 0.00020419414795469493\n",
      "Loss AMM, iter 690: 0.0001824043138185516\n",
      "Loss AMM, iter 691: 0.00013785470218863338\n",
      "Loss AMM, iter 692: 0.0001144525595009327\n",
      "Loss AMM, iter 693: 0.00017250087694264948\n",
      "Loss AMM, iter 694: 0.00015830369375180453\n",
      "Loss AMM, iter 695: 0.00013753538951277733\n",
      "Loss AMM, iter 696: 0.0001699209533398971\n",
      "Loss AMM, iter 697: 0.0001360135356662795\n",
      "Loss AMM, iter 698: 0.00014571122301276773\n",
      "Loss AMM, iter 699: 0.0001560340606374666\n",
      "Loss AMM, iter 700: 0.00012203252845210955\n",
      "Loss AMM, iter 701: 0.0001299682626267895\n",
      "Loss AMM, iter 702: 0.00011759829067159444\n",
      "Loss AMM, iter 703: 0.00016789641813375056\n",
      "Loss AMM, iter 704: 0.00016677254461683333\n",
      "Loss AMM, iter 705: 0.00012053141108481213\n",
      "Loss AMM, iter 706: 0.0001606237783562392\n",
      "Loss AMM, iter 707: 0.00015847750182729214\n",
      "Loss AMM, iter 708: 0.00012953998520970345\n",
      "Loss AMM, iter 709: 0.0001306662306888029\n",
      "Loss AMM, iter 710: 0.0001632429048186168\n",
      "Loss AMM, iter 711: 0.00014545601152349263\n",
      "Loss AMM, iter 712: 0.00013155261694919318\n",
      "Loss AMM, iter 713: 0.00012101214815629646\n",
      "Loss AMM, iter 714: 0.00012844805314671248\n",
      "Loss AMM, iter 715: 0.00013365155609790236\n",
      "Loss AMM, iter 716: 0.0001255196548299864\n",
      "Loss AMM, iter 717: 0.0001585287827765569\n",
      "Loss AMM, iter 718: 0.00011232138058403507\n",
      "Loss AMM, iter 719: 0.00010179638775298372\n",
      "Loss AMM, iter 720: 0.00012318285007495433\n",
      "Loss AMM, iter 721: 0.00014898947847541422\n",
      "Loss AMM, iter 722: 0.00013898417819291353\n",
      "Loss AMM, iter 723: 0.00013319261779543012\n",
      "Loss AMM, iter 724: 0.00011340522905811667\n",
      "Loss AMM, iter 725: 0.00011295706644887105\n",
      "Loss AMM, iter 726: 0.00014630574150942266\n",
      "Loss AMM, iter 727: 0.00010889904660871252\n",
      "Loss AMM, iter 728: 0.000116721261292696\n",
      "Loss AMM, iter 729: 0.0001287417981075123\n",
      "Loss AMM, iter 730: 0.0001424638758180663\n",
      "Loss AMM, iter 731: 0.00015316657663788646\n",
      "Loss AMM, iter 732: 0.00012394321674946696\n",
      "Loss AMM, iter 733: 0.00011585749598452821\n",
      "Loss AMM, iter 734: 0.00013079229393042624\n",
      "Loss AMM, iter 735: 0.00013411724648904055\n",
      "Loss AMM, iter 736: 0.00012083953333785757\n",
      "Loss AMM, iter 737: 0.00020270654931664467\n",
      "Loss AMM, iter 738: 0.00011560183338588104\n",
      "Loss AMM, iter 739: 0.00012675880861934274\n",
      "Loss AMM, iter 740: 0.00016085464449133724\n",
      "Loss AMM, iter 741: 0.00013474035949911922\n",
      "Loss AMM, iter 742: 0.00012325409625191242\n",
      "Loss AMM, iter 743: 0.00012190903362352401\n",
      "Loss AMM, iter 744: 0.00011317821918055415\n",
      "Loss AMM, iter 745: 0.00011518281098688021\n",
      "Loss AMM, iter 746: 0.0001120042652473785\n",
      "Loss AMM, iter 747: 0.0001484392414567992\n",
      "Loss AMM, iter 748: 0.00014431319141294807\n",
      "Loss AMM, iter 749: 0.00010457900498295203\n",
      "Loss AMM, iter 750: 0.00015204520605038851\n",
      "Loss AMM, iter 751: 0.00014764843217562884\n",
      "Loss AMM, iter 752: 0.0001190862967632711\n",
      "Loss AMM, iter 753: 0.00011039591481676325\n",
      "Loss AMM, iter 754: 0.00013365646009333432\n",
      "Loss AMM, iter 755: 0.00011037606600439176\n",
      "Loss AMM, iter 756: 0.00010121447121491656\n",
      "Loss AMM, iter 757: 0.0001519655779702589\n",
      "Loss AMM, iter 758: 8.557205001125112e-05\n",
      "Loss AMM, iter 759: 0.00012847337347920984\n",
      "Loss AMM, iter 760: 0.00011159273708472028\n",
      "Loss AMM, iter 761: 9.99513067654334e-05\n",
      "Loss AMM, iter 762: 0.0001294925605179742\n",
      "Loss AMM, iter 763: 0.00013439414033200592\n",
      "Loss AMM, iter 764: 0.00010847527300938964\n",
      "Loss AMM, iter 765: 0.00013173102342989296\n",
      "Loss AMM, iter 766: 0.00012822884309571236\n",
      "Loss AMM, iter 767: 0.00012843574222642928\n",
      "Loss AMM, iter 768: 0.00013521216169465333\n",
      "Loss AMM, iter 769: 0.00011488215386634693\n",
      "Loss AMM, iter 770: 0.0001377445150865242\n",
      "Loss AMM, iter 771: 9.885918552754447e-05\n",
      "Loss AMM, iter 772: 9.186821262119338e-05\n",
      "Loss AMM, iter 773: 0.00011262863699812442\n",
      "Loss AMM, iter 774: 0.00013836393191013485\n",
      "Loss AMM, iter 775: 0.00015567296941298991\n",
      "Loss AMM, iter 776: 0.0001120395609177649\n",
      "Loss AMM, iter 777: 0.00015774003986734897\n",
      "Loss AMM, iter 778: 0.0001094594263122417\n",
      "Loss AMM, iter 779: 9.136187873082235e-05\n",
      "Loss AMM, iter 780: 0.00014529391773976386\n",
      "Loss AMM, iter 781: 0.00011502159759402275\n",
      "Loss AMM, iter 782: 0.00011101541895186529\n",
      "Loss AMM, iter 783: 9.085887722903863e-05\n",
      "Loss AMM, iter 784: 0.00011159334098920226\n",
      "Loss AMM, iter 785: 0.0001027249381877482\n",
      "Loss AMM, iter 786: 9.952202526619658e-05\n",
      "Loss AMM, iter 787: 0.00013073920854367316\n",
      "Loss AMM, iter 788: 0.00011303908104309812\n",
      "Loss AMM, iter 789: 0.00011626421473920345\n",
      "Loss AMM, iter 790: 9.582487837178633e-05\n",
      "Loss AMM, iter 791: 0.0001272821828024462\n",
      "Loss AMM, iter 792: 0.00010948423732770607\n",
      "Loss AMM, iter 793: 0.00011428380821598694\n",
      "Loss AMM, iter 794: 9.842166036833078e-05\n",
      "Loss AMM, iter 795: 0.00010059487976832315\n",
      "Loss AMM, iter 796: 0.00012445646279957145\n",
      "Loss AMM, iter 797: 0.00013319426216185093\n",
      "Loss AMM, iter 798: 0.00010096694313688204\n",
      "Loss AMM, iter 799: 0.00010503373778192326\n",
      "Loss AMM, iter 800: 0.00011535636440385133\n",
      "Loss AMM, iter 801: 0.00010387733345851302\n",
      "Loss AMM, iter 802: 0.00012946010974701494\n",
      "Loss AMM, iter 803: 0.00010970896255457774\n",
      "Loss AMM, iter 804: 9.826692257774994e-05\n",
      "Loss AMM, iter 805: 8.854068437358364e-05\n",
      "Loss AMM, iter 806: 0.00010650233889464289\n",
      "Loss AMM, iter 807: 0.00012027530465275049\n",
      "Loss AMM, iter 808: 9.904341277433559e-05\n",
      "Loss AMM, iter 809: 0.0001221172569785267\n",
      "Loss AMM, iter 810: 8.280573092633858e-05\n",
      "Loss AMM, iter 811: 0.00011971047933911905\n",
      "Loss AMM, iter 812: 8.618692663731053e-05\n",
      "Loss AMM, iter 813: 9.438517736271024e-05\n",
      "Loss AMM, iter 814: 0.00010678773105610162\n",
      "Loss AMM, iter 815: 0.0001053366795531474\n",
      "Loss AMM, iter 816: 0.00012020955182379112\n",
      "Loss AMM, iter 817: 9.037221752805635e-05\n",
      "Loss AMM, iter 818: 0.00015207390242721885\n",
      "Loss AMM, iter 819: 9.171404963126406e-05\n",
      "Loss AMM, iter 820: 0.00011941233969992027\n",
      "Loss AMM, iter 821: 9.835280798142776e-05\n",
      "Loss AMM, iter 822: 9.67972882790491e-05\n",
      "Loss AMM, iter 823: 0.00010202384146396071\n",
      "Loss AMM, iter 824: 0.0001558171643409878\n",
      "Loss AMM, iter 825: 0.00010857131564989686\n",
      "Loss AMM, iter 826: 0.0001453222066629678\n",
      "Loss AMM, iter 827: 0.00013101415242999792\n",
      "Loss AMM, iter 828: 0.0001432570134056732\n",
      "Loss AMM, iter 829: 0.00012418703408911824\n",
      "Loss AMM, iter 830: 9.597164171282202e-05\n",
      "Loss AMM, iter 831: 0.00011631641973508522\n",
      "Loss AMM, iter 832: 7.688379992032424e-05\n",
      "Loss AMM, iter 833: 8.39884378365241e-05\n",
      "Loss AMM, iter 834: 9.031641093315557e-05\n",
      "Loss AMM, iter 835: 0.00011133297812193632\n",
      "Loss AMM, iter 836: 0.00010915540769929066\n",
      "Loss AMM, iter 837: 0.00012768743908964097\n",
      "Loss AMM, iter 838: 0.00011444287520134822\n",
      "Loss AMM, iter 839: 0.00011323531361995265\n",
      "Loss AMM, iter 840: 0.00010974922770401463\n",
      "Loss AMM, iter 841: 0.00011663811892503873\n",
      "Loss AMM, iter 842: 0.00010493089212104678\n",
      "Loss AMM, iter 843: 8.885352872312069e-05\n",
      "Loss AMM, iter 844: 0.00010516735346755013\n",
      "Loss AMM, iter 845: 0.00012449648056644946\n",
      "Loss AMM, iter 846: 0.000201402697712183\n",
      "Loss AMM, iter 847: 8.634869300294667e-05\n",
      "Loss AMM, iter 848: 8.520726260030642e-05\n",
      "Loss AMM, iter 849: 0.00011045263818232343\n",
      "Loss AMM, iter 850: 9.721802052808926e-05\n",
      "Loss AMM, iter 851: 0.00014725221262779087\n",
      "Loss AMM, iter 852: 9.346423030365258e-05\n",
      "Loss AMM, iter 853: 0.00011931702465517446\n",
      "Loss AMM, iter 854: 9.394861262990162e-05\n",
      "Loss AMM, iter 855: 0.00010485756502021104\n",
      "Loss AMM, iter 856: 9.251927258446813e-05\n",
      "Loss AMM, iter 857: 8.855645137373358e-05\n",
      "Loss AMM, iter 858: 8.55838370625861e-05\n",
      "Loss AMM, iter 859: 9.201250941259786e-05\n",
      "Loss AMM, iter 860: 9.498721192358062e-05\n",
      "Loss AMM, iter 861: 9.897776180878282e-05\n",
      "Loss AMM, iter 862: 0.00011305780935799703\n",
      "Loss AMM, iter 863: 9.839639824349433e-05\n",
      "Loss AMM, iter 864: 8.404561231145635e-05\n",
      "Loss AMM, iter 865: 9.356763621326536e-05\n",
      "Loss AMM, iter 866: 9.570344991516322e-05\n",
      "Loss AMM, iter 867: 9.104637865675613e-05\n",
      "Loss AMM, iter 868: 9.316182695329189e-05\n",
      "Loss AMM, iter 869: 0.00012114475248381495\n",
      "Loss AMM, iter 870: 0.0001316515263170004\n",
      "Loss AMM, iter 871: 0.0001398527529090643\n",
      "Loss AMM, iter 872: 9.129556565312669e-05\n",
      "Loss AMM, iter 873: 0.00012562781921587884\n",
      "Loss AMM, iter 874: 9.778451931197196e-05\n",
      "Loss AMM, iter 875: 9.239072824129835e-05\n",
      "Loss AMM, iter 876: 0.0001204393120133318\n",
      "Loss AMM, iter 877: 9.457964188186452e-05\n",
      "Loss AMM, iter 878: 8.03455404820852e-05\n",
      "Loss AMM, iter 879: 0.00010272162035107613\n",
      "Loss AMM, iter 880: 9.722698450786993e-05\n",
      "Loss AMM, iter 881: 9.945328201865777e-05\n",
      "Loss AMM, iter 882: 0.00010378961451351643\n",
      "Loss AMM, iter 883: 0.00012088434596080333\n",
      "Loss AMM, iter 884: 7.819147867849097e-05\n",
      "Loss AMM, iter 885: 9.270974987884983e-05\n",
      "Loss AMM, iter 886: 0.00011033255577785894\n",
      "Loss AMM, iter 887: 8.380514191230759e-05\n",
      "Loss AMM, iter 888: 9.289037552662194e-05\n",
      "Loss AMM, iter 889: 9.539853635942563e-05\n",
      "Loss AMM, iter 890: 9.797292295843363e-05\n",
      "Loss AMM, iter 891: 9.170441626338288e-05\n",
      "Loss AMM, iter 892: 0.00010034445585915819\n",
      "Loss AMM, iter 893: 0.0001080645524780266\n",
      "Loss AMM, iter 894: 8.36372419144027e-05\n",
      "Loss AMM, iter 895: 8.959817205322906e-05\n",
      "Loss AMM, iter 896: 0.00011416089546401054\n",
      "Loss AMM, iter 897: 8.527413592673838e-05\n",
      "Loss AMM, iter 898: 0.00010147538705496117\n",
      "Loss AMM, iter 899: 9.286296699428931e-05\n",
      "Loss AMM, iter 900: 9.250843868358061e-05\n",
      "Loss AMM, iter 901: 8.266109944088385e-05\n",
      "Loss AMM, iter 902: 0.00011599157005548477\n",
      "Loss AMM, iter 903: 0.00010816362919285893\n",
      "Loss AMM, iter 904: 8.98259604582563e-05\n",
      "Loss AMM, iter 905: 8.634367259219289e-05\n",
      "Loss AMM, iter 906: 9.907597996061668e-05\n",
      "Loss AMM, iter 907: 0.00011283857747912407\n",
      "Loss AMM, iter 908: 8.171711670001969e-05\n",
      "Loss AMM, iter 909: 9.598000178812072e-05\n",
      "Loss AMM, iter 910: 8.936093945521861e-05\n",
      "Loss AMM, iter 911: 0.00013272812066134065\n",
      "Loss AMM, iter 912: 8.586298645241186e-05\n",
      "Loss AMM, iter 913: 7.30796527932398e-05\n",
      "Loss AMM, iter 914: 0.00011735661246348172\n",
      "Loss AMM, iter 915: 0.00012256723130121827\n",
      "Loss AMM, iter 916: 6.49383437121287e-05\n",
      "Loss AMM, iter 917: 8.811146108200774e-05\n",
      "Loss AMM, iter 918: 0.00010711718641687185\n",
      "Loss AMM, iter 919: 8.128850458888337e-05\n",
      "Loss AMM, iter 920: 8.956019155448303e-05\n",
      "Loss AMM, iter 921: 9.167136158794165e-05\n",
      "Loss AMM, iter 922: 9.198522457154468e-05\n",
      "Loss AMM, iter 923: 0.0001087625787477009\n",
      "Loss AMM, iter 924: 8.161685400409624e-05\n",
      "Loss AMM, iter 925: 7.335888949455693e-05\n",
      "Loss AMM, iter 926: 0.00013515116006601602\n",
      "Loss AMM, iter 927: 0.0001014892477542162\n",
      "Loss AMM, iter 928: 8.78837236086838e-05\n",
      "Loss AMM, iter 929: 7.141615787986666e-05\n",
      "Loss AMM, iter 930: 7.957807974889874e-05\n",
      "Loss AMM, iter 931: 8.621730376034975e-05\n",
      "Loss AMM, iter 932: 0.00011779396299971268\n",
      "Loss AMM, iter 933: 8.430793968727812e-05\n",
      "Loss AMM, iter 934: 9.188585681840777e-05\n",
      "Loss AMM, iter 935: 8.343174704350531e-05\n",
      "Loss AMM, iter 936: 9.820557170314714e-05\n",
      "Loss AMM, iter 937: 9.366677113575861e-05\n",
      "Loss AMM, iter 938: 8.262205665232614e-05\n",
      "Loss AMM, iter 939: 9.063389734365046e-05\n",
      "Loss AMM, iter 940: 9.078254515770823e-05\n",
      "Loss AMM, iter 941: 9.99331969069317e-05\n",
      "Loss AMM, iter 942: 0.00011747194366762415\n",
      "Loss AMM, iter 943: 6.055660924175754e-05\n",
      "Loss AMM, iter 944: 8.531996718375012e-05\n",
      "Loss AMM, iter 945: 9.997206507250667e-05\n",
      "Loss AMM, iter 946: 7.46916193747893e-05\n",
      "Loss AMM, iter 947: 8.014126069610938e-05\n",
      "Loss AMM, iter 948: 8.099797560134903e-05\n",
      "Loss AMM, iter 949: 7.760532753309235e-05\n",
      "Loss AMM, iter 950: 6.414942617993802e-05\n",
      "Loss AMM, iter 951: 0.0001097048880183138\n",
      "Loss AMM, iter 952: 6.95071867085062e-05\n",
      "Loss AMM, iter 953: 0.00012715408229269087\n",
      "Loss AMM, iter 954: 7.366378849837929e-05\n",
      "Loss AMM, iter 955: 7.944885146571323e-05\n",
      "Loss AMM, iter 956: 7.825947250239551e-05\n",
      "Loss AMM, iter 957: 6.77666612318717e-05\n",
      "Loss AMM, iter 958: 7.420877227559686e-05\n",
      "Loss AMM, iter 959: 0.00011314172297716141\n",
      "Loss AMM, iter 960: 7.089979771990329e-05\n",
      "Loss AMM, iter 961: 8.170334331225604e-05\n",
      "Loss AMM, iter 962: 0.00010912055586231872\n",
      "Loss AMM, iter 963: 9.338327072327957e-05\n",
      "Loss AMM, iter 964: 6.758257222827524e-05\n",
      "Loss AMM, iter 965: 8.980248094303533e-05\n",
      "Loss AMM, iter 966: 6.72187888994813e-05\n",
      "Loss AMM, iter 967: 5.7914989156415686e-05\n",
      "Loss AMM, iter 968: 9.67281885095872e-05\n",
      "Loss AMM, iter 969: 8.351042924914509e-05\n",
      "Loss AMM, iter 970: 8.775793685344979e-05\n",
      "Loss AMM, iter 971: 0.0001083747047232464\n",
      "Loss AMM, iter 972: 8.030416211113334e-05\n",
      "Loss AMM, iter 973: 8.249769598478451e-05\n",
      "Loss AMM, iter 974: 8.949536277214065e-05\n",
      "Loss AMM, iter 975: 8.927474118536338e-05\n",
      "Loss AMM, iter 976: 7.327842467930168e-05\n",
      "Loss AMM, iter 977: 8.018528751563281e-05\n",
      "Loss AMM, iter 978: 9.954425331670791e-05\n",
      "Loss AMM, iter 979: 8.712567068869248e-05\n",
      "Loss AMM, iter 980: 8.47666451591067e-05\n",
      "Loss AMM, iter 981: 5.8490706578595564e-05\n",
      "Loss AMM, iter 982: 9.796366794034839e-05\n",
      "Loss AMM, iter 983: 7.92323553469032e-05\n",
      "Loss AMM, iter 984: 9.928841609507799e-05\n",
      "Loss AMM, iter 985: 7.05473285051994e-05\n",
      "Loss AMM, iter 986: 9.305042476626113e-05\n",
      "Loss AMM, iter 987: 7.972266030265018e-05\n",
      "Loss AMM, iter 988: 6.470977677963674e-05\n",
      "Loss AMM, iter 989: 7.867948443163186e-05\n",
      "Loss AMM, iter 990: 7.033489237073809e-05\n",
      "Loss AMM, iter 991: 9.077069262275472e-05\n",
      "Loss AMM, iter 992: 7.362120959442109e-05\n",
      "Loss AMM, iter 993: 6.643404776696116e-05\n",
      "Loss AMM, iter 994: 8.572975639253855e-05\n",
      "Loss AMM, iter 995: 8.182672172551975e-05\n",
      "Loss AMM, iter 996: 7.91178026702255e-05\n",
      "Loss AMM, iter 997: 5.4749049013480544e-05\n",
      "Loss AMM, iter 998: 0.00010063818626804277\n",
      "Loss AMM, iter 999: 8.22381698526442e-05\n",
      "Overfit complete\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Get the dictionary for feature visualization\n",
    "    vis_dict = {}\n",
    "\n",
    "    # Unpack config\n",
    "    root_dir = config[\"root_dir\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_workers = config[\"num_workers\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    learning_rate = config[\"lr\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Dataset and loader\n",
    "    dataloader = get_dataset(\n",
    "        root_dir=root_dir,\n",
    "        split=\"train\",\n",
    "        transform=None,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Res18FPNCEASC(config_path=config[\"config_path\"], num_classes=10)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "    \n",
    "    # Losses\n",
    "    l_amm = Lamm()\n",
    "\n",
    "    batch = next(iter(dataloader))\n",
    "\n",
    "    images = batch[\"image\"].to(device)\n",
    "    targets = {\n",
    "        \"boxes\": batch[\"boxes\"],\n",
    "        \"labels\": batch[\"labels\"],\n",
    "        \"image_id\": batch[\"image_id\"],\n",
    "        \"orig_size\": batch[\"orig_size\"],\n",
    "    }\n",
    "    # print(\"\\n🔍 Inspecting `targets` structure:\")\n",
    "    # for i in range(len(targets[\"boxes\"])):\n",
    "    #     print(f\"--- Sample {i} ---\")\n",
    "    #     print(f\"Image ID:         {targets['image_id'][i]}\")\n",
    "    #     print(f\"Original Size:    {targets['orig_size'][i]}\")\n",
    "    #     print(f\"Boxes shape:      {targets['boxes'][i].shape}\")  # [N_i, 4]\n",
    "    #     print(f\"Labels shape:     {targets['labels'][i].shape}\")  # [N_i]\n",
    "    #     print(f\"Boxes:            {targets['boxes'][i]}\")\n",
    "    #     print(f\"Labels:           {targets['labels'][i]}\")\n",
    "\n",
    "    vis_dict[\"image_id\"] = targets[\"image_id\"]\n",
    "\n",
    "    n_iters = 1000\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for n in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images, stage=\"train\")\n",
    "        (\n",
    "            cls_outs,\n",
    "            reg_outs,\n",
    "            soft_mask_outs,\n",
    "            sparse_cls_feats_outs,\n",
    "            sparse_reg_feats_outs,\n",
    "            dense_cls_feats_outs,\n",
    "            dense_reg_feats_outs,\n",
    "            feats,\n",
    "            anchors,\n",
    "        ) = outputs\n",
    "\n",
    "        if n == 24 or n == 49 or n ==99:\n",
    "            vis_dict[f\"{n}_mask\"] = [s.clone().detach() for s in soft_mask_outs]\n",
    "\n",
    "        # print(\"\\n🔍 Output shapes from model:\")\n",
    "        # for i in range(len(cls_outs)):\n",
    "        #     print(f\"--- FPN Level {i} ---\")\n",
    "        #     print(f\"cls_outs[{i}]:              {safe_shape(cls_outs[i])}\")\n",
    "        #     print(f\"reg_outs[{i}]:              {safe_shape(reg_outs[i])}\")\n",
    "        #     print(\n",
    "        #         f\"soft_mask_outs[{i}]:    {safe_shape(soft_mask_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_cls_feats[{i}]:      {safe_shape(sparse_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"sparse_reg_feats[{i}]:      {safe_shape(sparse_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_cls_feats[{i}]:       {safe_shape(dense_cls_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(\n",
    "        #         f\"dense_reg_feats[{i}]:       {safe_shape(dense_reg_feats_outs[i])}\"\n",
    "        #     )\n",
    "        #     print(f\"feats[{i}]:                 {safe_shape(feats[i])}\")\n",
    "        \n",
    "        # for i, anchor in enumerate(anchors):\n",
    "        #     print(f\"P{i+3} Anchors shape: {anchor.shape}\")\n",
    "\n",
    "        loss_amm = l_amm(\n",
    "            soft_mask_outs, targets[\"boxes\"], im_dimx=1333, im_dimy=800\n",
    "        )  \n",
    "    \n",
    "        print(f\"Loss AMM, iter {n}: {loss_amm.item()}\")\n",
    "\n",
    "        writer.add_scalar('AMM Loss/overfit',loss_amm.item(),n)\n",
    "\n",
    "        loss_amm.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    print('Overfit complete')\n",
    "\n",
    "    with open(\"visdrone_masks.pickle\",\"wb\") as f:\n",
    "        pickle.dump(vis_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022f9a4-3f1a-406f-99d1-37591a7767e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
